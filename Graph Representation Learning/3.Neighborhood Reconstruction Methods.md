<font color=LightCoral>**Problem:**</font>

<font color=LightBlue >**Solution:**</font>

<font color=Lime  >**Definition:**</font>

<font color=LightSalmon >**Remarks:**</font>

<font color=Aqua >**Lemma Theorem**</font>

<font color=Red >**Note:**</font>

<font color=DarkViolet >**Assumption:**</font>

<font color=HotPink >***Proof:***</font>

---

<center> <font size=6 ><b>3.Neighborhood Reconstruction Methods</b></font></center> 

---



| discrepancy  |  矛盾  | succinctly | 简洁地 | surge | 激增 |
| :----------: | :----: | :--------: | :----: | :---: | :--: |
|  asymmetric  | 不对称 |    akin    | 相似的 |       |      |
| intractable  | 棘手的 |            |        |       |      |
| transductive | 传导的 |            |        |       |      |



The goal of **node embeddings** is to encode nodes as low-dimensional vectors that summarize their graph position and the structure of their local graph neighborhood. 将节点投影到一个潜在空间中，该潜在空间中的几何关系对应于原始图形或网络中的关系(P29)



## 3.1 An Encoder-Decoder Perspective

本节介绍的节点嵌入方法参考论文(P30)。本节，将图形表示学习问题视为涉及两个关键操作：1.  an **encoder** model maps each node in the graph into a low-dimensional vector or embedding.  2.  a **decoder** model takes the low-dimensional node embeddings and uses them to reconstruct information about each node’s neighborhood in the original graph.

<div align=center>
<img src="/Users/guass/Desktop/markdown/图神经网络/Graph Representation Learning/3png1.png" width="500" height=""/>
</div>



### 3.1.1 The Encoder

The **encoder** is a function that maps nodes $v \in \mathcal{V}$ to vector embeddings $\pmb{z}_v \in \mathbb{R}^d$. 可以记
$$
ENC: \mathcal{V} \rightarrow \mathbb{R}^d
$$
meaning that the encoder takes <font color=Red >**node IDs**</font> as input to generate the node embeddings.

In most work on node embeddings, the encoder relies on what we call the <font color=Lime  >**shallow embedding**</font> approach, where this encoder function is simply an embedding lookup based on the node ID. In other words, we have that
$$
ENC(v) = \pmb{Z}[v]
$$
where $\pmb{Z} \in \mathbb{R}^{|\mathcal{V}| \times d}$ is a matrix containing the embedding vectors for all nodes and $\pmb{Z}[v]$ denotes the row of $\pmb{Z}$ corresponding to node $v$.

Shallow embedding can be generalized, for instance, the encoder can use node features or the local graph、 structure around each node as input to generate an embedding. (GNNs)



### 3.1.2 The Decoder

The role of the **decoder** is to reconstruct certain graph statistics from the node embeddings that are generated by the encoder. 例如给定$u$的embedding$\pmb{z}_u$, decoder可能是尝试去预测$u$的邻居节点集合$\mathcal{N}(u)$或者邻接向量$\pmb{A}[u]$

当得到多个点的decoders时，可以定义**pairwise decoders**，他有以下结构：
$$
DEC: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}^+
$$
Pairwise decoders can be interpreted as predicting the relationship or similarity between pairs of nodes.

Applying the pairwise decoder to a pair of embeddings $(\pmb{z}_u,\pmb{z}_v)$  results in the **reconstruction** of the relationship between nodes $u$ and $v$. The goal is optimize the encoder and decoder to minimize the reconstruction loss so that (最小化重建损失？)
$$
DEC(ENC(u),ENC(v)) = DEC(\pmb{z}_u,\pmb{z}_v) \approx \pmb{S}[u,v]
$$
这里假定$\pmb{S}[u,v]$ is a graph-based similarity measure between nodes. 例如预测两个点是不是邻居可以使用$\pmb{S}[u,v] \triangleq \pmb{A}[u,v]$也可以定义为在2.2节中讨论的几种统计量



### 3.1.3 Optimizing an Encoder-Decoder Model

为了重建目标(4)，the standard practice is to minimize an empirical reconstruction loss $\mathcal{L}$ over a set of training node pairs $\mathcal{D}$:
$$
\mathcal{L} = \sum_{(u,v) \in \mathcal{D}} \ell (DEC(\pmb{z}_u,\pmb{z}_v),\pmb{S}[u,v])
$$
$\ell$可能是mean-squared error or even a classification loss(cross entropy), 大多数的最小化上述公式的方法是随机梯度下降，也有使用矩阵因子分解的。



### 3.1.4  Overview of the Encoder-Decoder Approach

The key benefit of the encoder-decoder framework is that it allows one to succinctly define and compare different embedding methods based on (i) their decoder function, (ii) their graph-based similarity measure, and (iii) their loss function.（可以让我们多多尝试hh）

下表展示了一些比较著名的node embedding methods

<div align=center>
<img src="/Users/guass/Desktop/markdown/图神经网络/Graph Representation Learning/3png2.png" width="700" height=""/>
</div>

We will begin with a discussion of node embedding methods that are motivated by **matrix factorization approaches**  and that have close theoretical connections to **spectral clustering**. Following this, we will discuss more recent methods based on random walks, they also share close theoretical ties to spectral graph theory.

## 3.2 Factorization-based approaches



- **Laplacian eigenmaps(LE)**  (P33),定义decoder基于L2-distance between node embeddings:
  $$
  DEC(\pmb{z}_u,\pmb{z}_v) = \| \pmb{z}_u -\pmb{z}_v\|^2_2
  $$
  The loss function then weighs pairs of nodes according to their similarity in the graph:
  $$
  \mathcal{L} =  \sum_{(u,v) \in \mathcal{D}} DEC(\pmb{z}_u,\pmb{z}_v) \cdot \pmb{S}[u,v]
  $$
  $\mathcal{D}$是node pairs的集合.

  如果$\pmb{S}$的构建满足一些拉普拉斯矩阵的性质，then the node embeddings that minimize the loss in Equation (6) are identical to the solution for spectral clustering(2.3节的)，特别的，如果我们假定$\pmb{z}_u$为d维的，那么公式(6)的最优解是$d$ smallest eigenvectors of the Laplacian (excluding the eigenvector of all ones)

- **Inner-product methods** 
  $$
  DEC(\pmb{z}_u,\pmb{z}_v) = \pmb{z}_u^T \pmb{z}_v
  $$
  Here, we assume that the similarity between two nodes—e.g., the overlap between their local neighborhoods—is proportional to the dot product of their embeddings.

  Some examples of this style of node embedding algorithms include the **Graph Factorization (GF) approach**, **GraRep**, and **HOPE**. All three of these methods combine the inner product decoder (Equation 8) with the following mean-squared error:
  $$
  \mathcal{L} = \sum_{(u,v) \in \mathcal{D}} \| DEC(\pmb{z}_u,\pmb{z}_v) - \pmb{S}[u,v] \|_2^2
  $$
  他们主要的不同是如何定义$\pmb{S}[u,v]$，例如 the notion of node-node similarity or neighborhood overlap that they use.  GF使用邻接矩阵$\pmb{S} \triangleq \pmb{A}$, GraRep defines $\pmb{S}$ based on powers of the adjacency matrix, HOPE algorithm supports general neighborhood overlap measures(2.2节中任何一种方法)

  These methods are referred to as matrix-factorization approaches, since   their loss functions can be minimized using factorization algorithms,such as the **singular value decomposition (SVD).**

  Indeed, by stacking the node embeddings $\pmb{z}_u \in \mathbb{R}^d$ into a matrix $\pmb{Z} \in \mathbb{R}^{|\mathcal{V}|\times d}$the reconstruction objective for these approaches can be written as
$$
\mathcal{L} \approx \| \pmb{ZZ}^T - \pmb{S}\|_2^2
$$
​       which corresponds to a low-dimensional factorization of the node-node similarity matrix $\pmb{S}$



## 3.3 Random walk embeddings

上一节讨论的内积方法都采用节点相似性的确定性度量。它们通常定义$\pmb{S}$是邻接矩阵的多项式，使得node embedding在$\pmb{z}_u^T \pmb{z}_v \approx \pmb{S}[u,v]$时被最优化。

random walk embedding的创新点在于：对节点嵌入进行了优化，使两个节点具有相似的嵌入（如果它们倾向于在图上的短随机游动中同时出现）。

- **DeepWalk and Node2Vec**  同样该方法也是使用**shallow embedding** approach and an **inner-product decoder**. 主要的不同是这些方法 define the notions of **node similarity** and **neighborhood reconstruction.** 并不是直接重构邻接矩阵或者其行列式的函数，而是optimize embeddings to encode the statistics of random walks. 

  Mathematically, the goal is to learn embeddings so that the following (roughly) holds:
  $$
  DEC(\pmb{z}_u ,\pmb{z}_v)  \triangleq \frac{e^{\pmb{z}_u^T \pmb{z}_u}}{\sum_{v_k \in \mathcal{V}} e^{\pmb{z}_u^T \pmb{z}_k}} \approx p_{\mathcal{G},T} (v|u)
  $$
  where $ p_{\mathcal{G},T} (v|u)$  is the probability of visiting $v$ on a length-$T$ random walk starting at $u$, with $T$ usually defined to be in the range $T \in \{2,...,10\}$,<font color=Red >**Note:**</font> 再次强调公式(11)与 factorization-based approaches 公式(9)的不同点是 similarity measure in (11) is both stochastic and asymmetric.

  为了训练 random walk embeddings, the general strategy is to use the decoder from Equation (11) and minimize the following cross-entropy loss:
  $$
  \mathcal{L} = \sum_{(u,v) \in \mathcal{D}} -\log (DEC(\pmb{z}_u,\pmb{z}_v))
  $$
  这里$\mathcal{D}$表示训练集的random walks, which is generated by sampling random walks starting from each node. 例如，可以假定每个节点$u$的$N$个共现节点是从分布$(u,v) \sim p_{\mathcal{G},T} (v|u)$抽样得到.

  但(12)分子计算量巨大，时间复杂度为$O(|\mathcal{V}|)$, 总的时间复杂度为$O(|\mathcal{V}||\mathcal{D}|)$。 改进的方法很多，并且这是一个基本的不同与原始的DeepWalk and node2vec相比。

  DeepWalk使用**hierarchical softmax**去近似公式(11)，其中包含leveraging a binary-tree structure to accelerate the computation， 论文(P35)

  node2vec使用**noise contrastive**方法去近似公式(12)，where the normalizing factor is approximated using **negative samples** in the following way，论文(P35)
  $$
  \mathcal{L} = \sum_{(u,v) \in \mathcal{D}} - \log (\sigma(\pmb{z}^T_u \pmb{z}_v)) - \gamma \mathbb{E}_{v_n \sim P_n (\mathcal{V})} [\log (-\sigma(\pmb{z}_u^T\pmb{z}_{v_n}))]
  $$
  这里$\sigma$表示logistic function, $P_n(\mathcal{V})$ to denote a distribution over the set of nodes $\mathcal{V}$, 并且假定$\gamma >0$ 是一个超参数。实际中$P_n(\mathcal{V})$通常使用uniform distribution, and the expectation is approximated using Monte Carlo sampling.

  DeepWalk simply employs uniformly random walks to define $ p_{\mathcal{G},T} (v|u)$. 但node2vec更加灵活， the node2vec approach introduces hyperparameters that allow the random walk probabilities to smoothly interpolate between walks that are more akin to breadth-first search or depth-first search over the graph.

- **Large-scale information network embeddings (LINE)** The LINE approach does not explicitly leverage random walks, but it shares conceptual motivations with DeepWalk and node2vec.(论文P35)

  The basic idea in LINE is to combine two encoder decoder objectives. 

  The first objective aims to encode first-order adjacency information and uses the following decoder: 
  $$
  DEC(\pmb{z}_u,\pmb{z}_v) = \frac{1}{1+e^{-\pmb{z}_u^T\pmb{z}_v}}
  $$
  with an adjacency-based similarity measure (i.e.,$\pmb{S}[u,v] = \pmb{A}[u,v]$)

  The second objective is more similar to the random walk approaches.  It is the same decoder as Equation (11), but it is trained using the KL-divergence to encode two-hop adjacency information (i.e., the information in $\pmb{A}^2$）因此，LINE在概念上与node2vec和DeepWalk相关。它使用概率解码器和概率损失函数（基于KL散度）。然而，它不是采样随机游动，而是显式地重建一阶和二阶邻域信息。

- **Additional variants of the random-walk idea** 随机游走方法的一个优点就是其可拓展性与修改性质。比如： The random walks that “skip” over nodes, which generates a similarity measure similar to GraRep (discussed in Section 3.2). 也有基于节点之间的结构关系而非邻域信息定义随机游动，邻域信息生成节点嵌入，对图中的结构角色进行编码。(P36)



### 3.3.1  Random walk methods and matrix factorization

It can be shown that random walk methods are actually closely related to matrix factorization approaches(P36)

Suppose we define the following matrix of node-node similarity values:
$$
\pmb{S}_{DW} = \log \Big( \frac{vol(\mathcal{V})}{T} \Big( \sum_{t=1}^T \pmb{P}^t\Big) \pmb{D}^{-1}  \Big) - \log(b)
$$
$vol(\mathcal{V} ) = \sum_{u \in \mathcal{V}} d_u$, where $b$ is a constant and $\pmb{P = D^{-1}A}$, 在这种情况下the embeddings $\pmb{Z}$ learned by DeepWalk satisfy:
$$
\pmb{Z}^T \pmb{Z} \approx \pmb{S}_{DW}
$$
Interestingly, we can also decompose the interior part of Equation (15) as
$$
\Big(\sum_{t=1}^T \pmb{P}^t\Big) \pmb{D}^{-1} = \pmb{D}^{-\frac{1}{2}} \Big(\pmb{U}\Big(\sum_{t=1}^T \Lambda ^t\Big)\pmb{U}^T\Big)\pmb{D}^{-\frac{1}{2}}
$$
where $\pmb{U\Lambda U}^T = \pmb{L}_{sym}$ is the eigendecomposition of the symmetric normalized Laplacian. 

这揭示了使用DeepWalk的嵌入学习事实上与 spectral clustering embeddings关系很大。

The key difference is that the DeepWalk embeddings control the influence of different eigenvalues through *T*, i.e., the length of the random walk.

更多二者关系的论文(P36)





## 3.4  Limitations of Shallow Embeddings



1. The first issue is that shallow embedding methods do not share any parameters between nodes in the encoder, since the encoder directly optimizes a unique embedding vector for each node.从统计角度来看，参数共享可以提高学习效率，同时也是一种强大的正则化形式。从计算的角度来看，缺少参数共享意味着浅嵌入方法中的参数数量必然会随着$O(|\mathcal{V}|)$
2.  They do not leverage node features in the encoder.
3. Shallow embedding methods are inherently **transductive**，这些方法只能为训练阶段出现的节点生成嵌入。除非执行额外优化以了解这些节点的嵌入，否则无法为训练阶段后观察到的新节点生成嵌入。这种限制阻止了浅层嵌入方法在归纳应用中的应用，这涉及到在训练后推广到看不见的节点。**（这个问题确实很严重啊）**(P37)

解决方法GNNs，见以后章节















































