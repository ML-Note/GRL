<font color=LightCoral>**Problem:**</font>

<font color=LightBlue >**Solution:**</font>

<font color=Lime  >**Definition:**</font>

<font color=LightSalmon >**Remarks:**</font>

<font color=Aqua >**Lemma Theorem**</font>

<font color=Red >**Note:**</font>

<font color=DarkViolet >**Assumption:**</font>

<font color=HotPink >***Proof:***</font>

---

<center> <font size=6 ><b>5.6.7.Graph Neural Networks</b></font></center> 

---



|  formalism   |   形式主义   |  desideratum  | 需求量 |     defer     | 推迟 |
| :----------: | :----------: | :-----------: | :----: | :-----------: | :--: |
|     bulk     |    大体积    | instantiation | 实例化 |     span      | 跨度 |
|  succinctly  |    简洁地    |    drastic    | 极端的 |  conjunction  | 结合 |
| catastrophic |   灾难性的   |  intractable  | 棘手的 |  disentangle  | 理顺 |
|  canonical   |    典型的    |  promiscuous  | 杂乱的 |    augment    | 加强 |
|   drastic    |    激烈的    | off-the-shelf | 现成的 |    combat     | 减轻 |
|   incident   |  发生的事情  | transductive  | 传导的 |  coarsening   | 粗化 |
|  inductive   |    归纳的    |  fine-tuning  |  微调  | granularities | 粒度 |
|    usher     |     引入     |  counterpart  | 对应方 |  prominence   | 突出 |
| detrimental  |    有害的    | underpinning  |  基础  |    shuffle    | 洗牌 |
|  intriguing  |    有趣的    |    modulus    |  模数  |  isomorphism  | 同构 |
|  disparate   |  完全不同的  | underpinning  |  基础  |   amplitude   | 振幅 |
|  sinusoidal  |   正弦曲线   |    commute    |  交换  |     grain     | 粮食 |
|   underlie   | 构成..的基础 |  interleave   |  插入  |    hashing    | 散列 |
|   agnostic   |  不可知论者  |               |        |               |      |
| penultimate  |  倒数第二的  |               |        |               |      |
|              |              |               |        |               |      |
|              |              |               |        |               |      |
|              |              |               |        |               |      |
|              |              |               |        |               |      |



# 5. The Graph Neural Network Model

之前我们 used a **shallow embedding**  approach to generate representations of nodes, where we simply optimized a unique embedding vector for each node. 这一章将会 focus to more complex encoder models.

To define a deep neural network over general graphs, we need to define a new kind of deep learning architecture.(不同是CNN与RNN)

> **Permutation invariance and equivariance** 一般使用深度学习网络，使用邻接矩阵作为他的输入. 例如：为了生成整个图的嵌入，我们可以简单地展平邻接矩阵，并将结果反馈给多层感知器（MLP）：
> $$
> \pmb{z}_\mathcal{G} = MLP(\pmb{A}[1] \oplus\pmb{A}[2] ... \oplus\pmb{A}[|\mathcal{V}|] )
> $$
> where $\pmb{A}[i] \in \pmb{R}^{|\mathcal{V}|}$  denotes a row of the adjacency matrix and we use $\oplus$ to denote vector concatenation.
>
> The issue with this approach is that it **depends on the arbitrary ordering of nodes that we used in the adjacency matrix**.  也就是说这个模型不是**permutation invariant**. 任何作用在邻接矩阵$\pmb{A}$上的函数$f$都应该满足两个性质
>
> - Permutation Invariance: $f(\pmb{PAP}^T) = f(\pmb{A})$
>
> - Permutation Equivariance: $f(\pmb{PAP^T}) = \pmb{P}f(\pmb{A})$  <font color=Red >**这确定没标错？**</font>
>
>  $\pmb{P}$：permutation matrix. Permutation equivariance means that the output of $f$ is permuted in an consistent way when we permute the adjacency matrix. 
>



## 5.1 Neural Message Passing

GNN的几种基本模型论文(P48)

Regardless of the motivation, the defining feature of a GNN is that it uses a form of **neural message passing** in which vector messages are exchanged between nodes and updated using neural networks.

The bulk of this chapter will describe how we can take an input graph $\mathcal{G} = (\mathcal{V},\mathcal{E})$, along with a set of node features $\pmb{X} \in \mathbb{R}^{d \times |\mathcal{V}|}$, , and use this information to generate node embeddings $\pmb{z}_u,\forall u \in \mathcal{V}$

### 5.1.1 Overview of the Message Passing Framework

在GNN的每一轮message-passing中, a hidden embedding $\pmb{h}_u^{(k)}$ corresponding to each node $u \in \mathcal{V}$  is updated according to information aggregated from $u$'s graph neighborhood $\mathcal{N}(u)$.消息传递的更新可以写为
$$
\pmb{h}_u^{(k+1)} = \text{UPDATE}^{(k)} \Big(\pmb{h}_u^{(k)},AGGREGATE^{(k)}(\{\pmb{h}_v^{(k)},\forall v \in \mathcal{N}(u)\} )\Big) = \text{UPDATE}^{(k)} \Big( \pmb{h}_u^{(k)},\pmb{m}_{\mathcal{N}(u)}^{(k)}\Big)
$$

<div align=center>
<img src="/Users/guass/Desktop/markdown/图神经网络/Graph Representation Learning/5png1.png" width="600" height=""/>
</div>

where  UPDATE and AGGREGATE are arbitrary differentiable functions (比如神经网络). $\pmb{m}_{\mathcal{N}(u)}$ is the "message"  that is aggregated from $u$’s graph neighborhood $\mathcal{N}(u)$. 其中$\pmb{h}_u^{(0)} = \pmb{x}_u,\forall u \in \mathcal{V}$, 更新了$K$次之后就得到每个点的embedding
$$
\pmb{z}_u  = \pmb{h}_u^{(K)},\forall u \in \mathcal{V}
$$
Note that since the AGGREGATE function takes a *set* as input, GNNs defined in this way are permutation equivariant by design.

> **Node Features** 不像之前的shallow embedding. GNN框架要求我们有node features $\pmb{x}_u,\forall u \in \mathcal{V}$ 作为模型的输入. 如果没有node features 可以使用2.1节提出的节点统计量或者使用**identity features**,  where we associate each node with a one-hot indicator feature, which uniquely identifies that node. identity features方法的缺陷是，如果有新的节点，无法表示。



### 5.1.2 Motivations and Intuitions

The basic intuition behind the GNN **message-passing framework** is straight forward: at each iteration, every node aggregates information from its local neighborhood, and as these iterations progress each node embedding contains more and more information from further reaches of the graph. 

What kind of “information” do these node embeddings actually encode? 

- **structural information** about the graph
- **feature-based information**



### 5.1.3 The Basic GNN

更好的理解公式(2)，下面介绍其中一个实例化(两篇早期论文P51). The basic GNN message passing is defined as
$$
\pmb{h}_{u}^{(k)} = \sigma \Big(\pmb{W}_{self}^{(k)} \pmb{h}_{u}^{(k-1)}+\pmb{W}_{neigh}^{(k)} \sum_{v \in \mathcal{N}(u)} \pmb{h}_v^{(k-1)}+ \pmb{b}^{(k)}\Big)
$$
where $\pmb{W}_{self}^{(k)},\pmb{W}_{neigh}^{(k)} \in \mathbb{R}^{d^{(k)}\times d^{(k-1)}}$ are trainable parameter matrices and $\sigma$ denotes an elementwise non-linearity 

<font color=Red >**Note:**</font> 本书 we use superscripts to differentiate parameters, embeddings, and dimensionalities in different layers of the GNN.

We can equivalently define the basic GNN through the UPDATE and AGGREGATE functions:
$$
\pmb{m}_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)} \pmb{h}_v \\
UPDATE(\pmb{h}_u,\pmb{m}_{\mathcal{N}(u)}) = \sigma (\pmb{W}_{self}\pmb{h}_u+\pmb{W}_{neigh}\pmb{m}_{\mathcal{N}(u)})
$$
where we recall that we use
$$
\pmb{m}_{\mathcal{N}(u)} = AGGREGATE^{(k)} (\{\pmb{h}_v^{(k)},\forall v \in \mathcal{N}(u)\})
$$
<font color=Red >**Note:**</font>  also that we omitted the superscript denoting the iteration in the above equations, which we will often do for notational brevity.

> **Node vs. graph-level equations**:我们定义message-passing的核心操作是在节点的水平上，也可以简洁的写为graph-level euqations. we can write the graph-level definition of the model as follows:
> $$
> \pmb{H}^{(k)} = \sigma \Big( \pmb{AH}^{(k-1)} \pmb{W}_{neigh}^{(k)}+\pmb{H}^{(k-1)}\pmb{W}_{self}^{(k)}\Big)
> $$
> where $\pmb{H}^{(k)} \in \mathbb{R}^{|\mathcal{V}| \times d}$ denotes the matrix of node representations at layer $t$ in the GNN. $\pmb{A}$表示邻接矩阵.为了简洁省略掉了bias.
>
> 虽然这种图级表示不容易适用于所有GNN模型，如我们下面讨论的基于注意的模型，但它通常更简洁，并且还强调了使用少量稀疏矩阵操作可以有效实现多少GNN。



### 5.1.4 Message Passing with Self-loops

As a simplification of the neural message passing approach, it is common to add self-loops to the input graph and omit the explicit update step. In this approach we define the message passing simply as
$$
\pmb{h}_u^{(k)} = AGGREGATE(\{\pmb{h}_v^{(k-1)},\forall v \in \mathcal{N}(u) \cup \{u\}\})
$$
这种方法的好处是，我们不再需要定义显式更新函数，因为更新是通过聚合方法隐式定义的。以这种方式简化消息传递通常可以缓解过度拟合，但它也严重限制了GNN的表达能力，因为来自节点邻居的信息无法与来自节点本身的信息区分开来。

解决以上问题：增加self-loops, which gives the following graph-level update:
$$
\pmb{H}^{(t) } = \sigma \Big((\pmb{A+I}) \pmb{H}^{(t-1)} \pmb{W}^{(t)}\Big)
$$
以下将称为**self-loop GNN approach**



## 5.2  Generalized Neighborhood Aggregation

公式(4)所展示的GNN模型能够获得很好的效果， 但the basic GNN can be improved upon and generalized in many ways.  Here, we discuss how the AGGREGATE operator can be generalized and improved upon.

### 5.2.1 Neighborhood Normalization

公式(5)是最基本的neighborhood aggregation operation. 它的问题是对于节点度不稳定且高度敏感. 如果一个节点的度很大那么$\|\sum_{v \in \mathcal{N}(u)} \pmb{h}_v\|$也会变得很大。

最简单的改进
$$
\pmb{m}_{\mathcal{N}(u)} = \frac{\sum_{v \in \mathcal{N}(u)}\pmb{h}_v}{|\mathcal{N}(u)|}
$$
另一种改进**symmetric normalization**
$$
\pmb{m}_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)}\frac{\pmb{h}_v}{\sqrt{|\mathcal{N}(u)||\mathcal{N}(v)|}}
$$
以上方法在citation graph中比较好用。另外，对称归一化也可以基于谱图理论进行。特别是，将对称归一化聚合（公式11）与基本GNN更新函数（公式5）结合起来，可以得到谱图卷积的一阶近似值，



<font color=Lime  >**Graph convolutional networks (GCNs)**</font>—employs the symmetric-normalized aggregation as well as the self-loop update approach. The GCN model thus defines the message passing function as(P53)
$$
\pmb{h}_{u}^{(k)} = \sigma \Big(\pmb{W}^{(k)} \sum_{v \in \mathcal{N}(u)  \cup \{u\}} \frac{\pmb{h}_v}{\sqrt{|\mathcal{N}(u)||\mathcal{N}(v)|}}\Big)
$$

> **To normalize or not to normalize?**  合适的normalize可以有稳定性和更好的性能，但也会丢失信息。比如：after normalization, it can be hard (or even impossible) to use the learned embeddings to distinguish between nodes of different degrees, and various other structural graph features can be obscured by normalization.
>
> 这个还是要具体的问题具体讨论.



### 5.2.2 Set Aggregators

Is there perhaps something more sophisticated than just summing over the neighbor embeddings?

The neighborhood aggregation operation is fundamentally a **set function.**  Given a set of neighbor embeddings $\{\pmb{h}_v,\forall v \in \mathcal{N}(u)\}$ and must map this set to a single vector $\pmb{m}_{\mathcal{N}(u)}$. Any aggregation function we define must thus be **permutation invariant**.



<font color=Lime  >**Set pooling**</font>

One principled approach to define an aggregation function is based on the theory of **permutation invariant neural networks.**  An aggregation function with the following form is a **universal set function approximator**:(P54)
$$
\pmb{m}_{\mathcal{N}(u)} = MLP_{\theta}\Big(\sum_{v \in \mathcal{N}(u)} MLP_{\phi} (\pmb{h}_v)\Big)
$$
$MLP_{\theta}$ to denote an arbitrarily deep multi-layer perceptron parameterized by some trainable parameters  $\theta$

Any permutation-invariant function that maps a set of embeddings to a single embedding can be approximated to an arbitrary accuracy by a model following Equation (13). 这么猛？

<font color=DarkViolet >**Note:**</font> 上述公式  employs a sum of the embeddings after applying the first MLP. 也可以将sum替换掉，比如使用 element-wise maximum or minimum <font color=Red >**到底是(13)整个就可以接近任何permutation-invariant function，还是换了sum之后也可以达到任何，也就是括号里面的对于逼近的影响是什么**</font>

It is also common to combine models based on Equation (13) with the normalization approaches discussed in Section 5.2.1, as in the **GraphSAGE-pool approach**(P55)

基于等式(13)的集合池方法通常会导致性能的小幅度提高，尽管它们也会增加过度拟合的风险，这取决于所使用的MLP的深度。通常使用只有一个隐藏层的MLP。

<font color=Lime  >**Janossy pooling**</font>

Janossy pooling employs a different approach entirely: instead of using a permutation invariant reduction (e.g., a sum or mean), we apply a **permutation-sensitive function** and average the result over many possible permutations.

$\pi_i \in \Pi$表示一个permutation function将集合$\{\pmb{h}_v, \forall v \in \mathcal{N}(u)\}$ 映射到一个特定序列$(\pmb{h}_{v_1},\pmb{h}_{v_2}...,\pmb{h}_{v_{|\mathcal{N}(u)|}})_{\pi_i}$. The Janossy pooling approach then performs neighborhood aggregation by
$$
\pmb{m}_{\mathcal{N}(u) } = MLP_{\theta} \Bigg(\frac{1}{|\Pi|}\sum_{\pi \in \Pi} \rho_{\phi}(\pmb{h}_{v_1},\pmb{h}_{v_2}...,\pmb{h}_{v_{|\mathcal{N}(u)|}})_{\pi_i}\Bigg)
$$
$\Pi$表示a set of permutations and $\rho_{\phi}$ is a permutation-sensitive function.例如可以定义为LSTM

如果公式(14)中的置换$\Pi$集等于所有可能的置换，则公式(14)中的聚合器也是集合的通用函数近似器，但将所有可能的permutation相加通常比较棘手，因此Janossy pooling使用以下两种方法中的一种

1. Sample a random subset of possible permutations during each application of the aggregator, and only sum over that random subset.
2. Employ **canonical** ordering of the nodes in the neighborhood set;  e.g., order the nodes in descending order according to their degree, with ties broken randomly.  <font color=Red >**这种方法为什么可以是permutation**</font>

一篇论文比较了这些方法(P56)



### 5.2.3 Neighborhood Attention

The basic idea is to assign an attention weight or importance to each neighbor, which is used to weigh this neighbor’s influence during the aggregation step.  The first GNN model to apply this style of attention was  **Graph Attention Network (GAT)**(P56)
$$
\pmb{m}_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)} \alpha_{u,v} \pmb{h}_v
$$
$\alpha_{u,v}$就是衡量重要性程度的。  In the original GAT paper, the attention weights are defined as
$$
\alpha_{u,v} = \frac{\exp(\pmb{a}^T[\pmb{Wh}_u \oplus \pmb{Wh}_v])}{\sum_{v' \in \mathcal{N}(u)}\exp (\pmb{a}^T[\pmb{Wh}_u \oplus \pmb{Wh}_{v'}])}
$$
where $\pmb{a}$ is a trainable attention vector, $\pmb{W}$ is a trainable matrix, and $\oplus$ denotes the concatenation operation.(P56)

Popular variants of attention include the bilinear attention model
$$
\alpha_{u,v} = \frac{\exp(\pmb{h}_u^T\pmb{Wh}_v)} {\sum_{v' \in \mathcal{N}(u)}\exp(\pmb{h}_u^T\pmb{Wh}_{v'})}
$$
as well as variations of attention layers using MLPs, e.g.,
$$
\alpha_{u,v} = \frac{\exp(MLP(\pmb{h}_u,\pmb{h}_v))} {\sum_{v' \in \mathcal{N}(u)}\exp(MLP(\pmb{h}_u,\pmb{h}_{v'}))}
$$
where the MLP is restricted to a scalar output.

也可以使用 **transformer**的结构.  In this approach, one computes $K$ distinct attention weights $\alpha_{u,v,k}$ using independently parameterized attention layers. Usually with a linear projection followed by a concatenation operation,例如
$$
\pmb{m}_{\mathcal{N}(u)} = [\pmb{a}_1 \oplus \pmb{a}_2 \oplus,..., \oplus \pmb{a}_K ] \\
\pmb{a}_k =  \pmb{W}_k \sum_{v \in \mathcal{N}(u)}  \alpha_{u,v,k} \pmb{h}_v
$$
where the attention weights $\alpha_{u,v,k}$ or each of the $K$ attention heads can be computed using any of the above attention mechanisms.

> **Graph attention and transformers** 相关论文(P57) 以及GNN的attention transformer 方法，需要看论文和查阅更多资料



## 5.3 Generalized Update Methods

> **Over-smoothing and neighbourhood influence** 
>
> The essential idea of **over-smoothing** is that after several iterations of GNN message passing, the representations for all the nodes in the graph can become very similar to one another.
>
> This issue of over-smoothing in GNNs can be formalized by defining the influence of each node’s input feature $\pmb{h}_u^{(0)} = \pmb{x}_u$ on the final layer embedding of all the other nodes in the graph i.e. $\pmb{h}_v^{(K)},\forall v \in \mathcal{V}$. We can quantify the influence of node $u$ on node $v$ in the GNN by examining the magnitude of the corresponding Jacobian matrix
> $$
> I_{K}(u,v) = \pmb{1}^T \Bigg(\frac{\part \pmb{h}_v^{(K)}}{\part \pmb{h}_u^{(0)}}\Bigg) \pmb{1}
> $$
> where $\pmb{1}$ is a vector of all ones. $I_{K}(u,v) $ 衡量了how much the initial embedding of node $u$ influences the final embedding of node $v$ in the GNN.
>
> <font color=Aqua >**Theorem 3**</font>  *For any GNN model using a self-loop update approach and an aggregation function of the form*
> $$
> AGGREGATE(\{\pmb{h}_v,\forall v \in \mathcal{N}(u) \cup \{u\}\}) = \frac{1}{f_n(|\mathcal{N}(u) \cup \{u\}|)} \sum_{v \in \mathcal{N}(u) \cup \{u\}} \pmb{h}_v
> $$
> *where* $f:\mathbb{R}^+ \rightarrow \mathbb{R}^+$  *is an arbitrary differentiable normalization function, we have that*
> $$
> I_K (u,v) \propto  p_{\mathcal{G},K}(u|v)
> $$
> *where* $p_{\mathcal{G},K}(u|v)$ *denotes  the probability of visiting node $v$ on a length-$K$ random walk starting from node* $u$
>
>  An important consequence of this, however, is that as $K \rightarrow \infty$ the influence of every node approaches the stationary distribution of random walks over the graph, meaning that local neighborhood information is lost. 另外在现实图中，which contain high-degree nodes and resemble so-called “expander” graphs. It only takes $k = O(\log(|\mathcal{V}|))$ steps for the random walk starting from any node to converge to an almost-uniform distribution
>
> As more layers are added we lose information about local neighborhood structures and our learned embeddings become over-smoothed, approaching an almost-uniform distribution.



### 5.3.1 Concatenation and Skip-Connections

直观地说，在消息传递期间从节点邻居聚合的信息开始主导更新的节点表示的情况下，我们可以预期过度平滑。这种情况下, the updated node representations $\pmb{h}_u^{(k+1)}$ will depend too strongly on the incoming message aggregated from the neighbors $\pmb{m}_{\mathcal{N}(u)}$ at the expense of the node representations from the previous layers  $\pmb{h}_u^{(k)}$ 

A natural way to alleviate this issue is to use **vector concatenations** or **skip connections**,

Using $UPDATE_{base}$ to denote the base update function that we are building upon 公式(5), and we will define various skip-connection updates on top of this base function.

The concatenation-based skip connection was proposed in the GraphSAGE framework(P60). One of the **simplest skip connection updates** employs a concatenation to preserve more node-level information during message passing: 
$$
UPDATE_{concat}(\pmb{h}_u ,\pmb{m}_{\mathcal{N}(u)}) = [UPDATE_{base}(\pmb{h}_u,\pmb{m}_{\mathcal{N}(u)}) \oplus \pmb{h}_u]
$$
**Linear interpolation method**(P60)
$$
UPDATE_{interpolate}(\pmb{h}_u ,\pmb{m}_{\mathcal{N}(u)}) = \pmb{\alpha}_1 \circ UPDATE_{base}(\pmb{h}_u,\pmb{m}_{\mathcal{N}(u)}) + \pmb{\alpha}_2 \odot \pmb{h}_u
$$
where $\pmb{\alpha}_1,\pmb{\alpha}_2 \in [0,1]^d$ are gating vectors with $\pmb{\alpha}_1 = \pmb{1-\alpha}_2$ and $\circ$ denotes elementwise multiplication.

<font color=Red >**后面这个$\odot$应该是$\circ$**</font> The gating parameters $\pmb{\alpha}_1$ can be learned jointly with the model in a variety of ways.(P60有例子) 这一节参考的是CV中的CNN



### 5.3.2 Gated Updates

 这一节从RNNs中得到启发.

One of the earliest GNN architectures defines the update function as(P61)
$$
\pmb{h}_u^{(k)} = GRU(\pmb{h}_u^{(k-1)},\pmb{m}_{\mathcal{N}(u)}^{(k)})
$$
where GRU denotes the update equation of the gated recurrent unit (GRU) cell (P61)

Other approaches have employed updates based on the LSTM architecture(P61). 实际上，研究人员通常也在GNN的消息传递层之间共享更新函数的参数。一般来说，它们最适用于GNN应用，其中预测任务需要对图形的全局结构进行复杂推理，例如程序分析或组合优化应用 (P61)。



### 5.3.3 Jumping Knowledge Connections

In the preceding sections, we have been implicitly assuming that we are using the output of the final layer of the GNN. 也就是使用$\pmb{z}_u$作为最终输出
$$
\pmb{z}_u = \pmb{h}_u^{(K)},\forall u \in \mathcal{V}
$$
The limitations of this strategy motivated much of the need for residual and gated updates to limit over-smoothing. 

一个策略是 improve the quality of the final node representations is to simply leverage the representations *at each layer of message passing*， 在这种方法中定义最终的节点表示$\pmb{z}_u$为
$$
\pmb{z}_u = f_{JK}(\pmb{h}_u^{(0)} \oplus \pmb{h}_u^{(1) } \oplus ...\oplus \pmb{h}_u^{(K)})
$$
$f_{JK}$是任意可导函数，此方法称为 **jumping knowledge (JK) connections** (P62) 在许多应用程序中，函数$f_{JK}$可以简单地定义为标识函数，这意味着我们只是将每个层的节点嵌入连接起来(论文中也尝试了其他函数)。 This approach often leads to consistent improvements across a wide-variety of tasks and is a generally useful strategy to employ.



## 5.4 Edge Features and Multi-relational GNNs



### 5.4.1  Relational Graph Neural Networks

In this approach we augment the aggregation function to accommodate multiple relation types by specifying a separate transformation matrix per relation type:(P62)
$$
\pmb{m}_{\mathcal{N}(u)} = \sum_{\tau \in \mathbb{R}} \sum_{v \in \mathcal{N}_\tau (u)} \frac{\pmb{W}_\tau \pmb{h}_v}{f_n(\mathcal{N}(u),\mathcal{N}(v))}
$$
$f_n$ is a normalization function that can depend on both the neighborhood of the node $u$ as well as the neighbor $v$ being aggregated over.

总的来说，RGCN中的多关系聚合因此类似于规范化的基本GNN方法，但我们分别聚合不同边缘类型的信息。



<font color=Aqua >**Parameter sharing**</font>

One drawback of the naive RGCN approach is the drastic increase in the number of parameters.  A scheme to combat this issue by parameter sharing with basis matrices, where
$$
\pmb{W}_\tau = \sum_{i=1}^b \alpha_{i,\tau} \pmb{B}_i
$$
In this basis matrix approach, all the relation matrices are defined as linear combinations of $b$ basis matrices $\pmb{B}_1,...,\pmb{B}_b$ and the only relation-specific parameters are the $b$ combination weights $\alpha_{1,\tau},...,$$\alpha_{b,\tau}$ for each relation $\tau$. In this basis sharing approach, we can thus rewrite the full aggregation function as
$$
\pmb{m}_{\mathcal{N}(u)} = \sum_{\tau \in \mathbb{R}} \sum_{v \in \mathcal{N}_\tau (u)} \frac{\pmb{\alpha } \times_1 \mathcal{B} \times_2 \pmb{h}_v}{f_n(\mathcal{N}(u),\mathcal{N}(v)) }
$$
where $\mathcal{B} =(\pmb{B}_1,...,\pmb{B}_b)$ is a tensor formed by stacking the basis matrices. $\times_i$ denotes a tensor product along mode $i$

<font color=Aqua >**Extensions and variations**</font>

RGCN体系结构可以以多种方式扩展，通常，we refer to approaches that define separate aggregation matrices per relation as relational graph neural networks. 有无参数共享方法与将RGCN风格的聚合与注意力相结合的方法(P63)



### 5.4.2  Attention and Feature Concatenation

The relational GNN approach, where we define a separate aggregation parameter per relation. To accommodate cases where we have more general forms of edge features, we can leverage these features in attention or by concatenating this information with the neighbor embeddings during message passing. 比如以下形式
$$
\pmb{m}_{\mathcal{N}(u)} = AGGREGATE_{base}(\{\pmb{h}_v \oplus \pmb{e}_{(u,\tau,v)},\forall v \in \mathcal{N}(u)\})
$$
where $\pmb{e}_{u,\tau,v}$ denotes an arbitrary vector-valued feature for the edge $(u,\tau,v)$ (P63)

## 5.5 Graph Pooling



The neural message passing approach produces a set of **node** embeddings, but what if we want to make predictions at the **graph** level?

之前学的都是$\pmb{z}_u,\forall u \in \mathcal{V}$, but what if we to learn an embedding $\pmb{z}_\mathcal{G}$ for the entire graph $\mathcal{G}$. 这种任务称为

**graph pooling**,  since our goal is to pool together the node embeddings in order to learn an embedding of the entire graph.

<font color=Aqua >**Set pooling approaches**</font>

We want to design a pooling function $f_p$, which maps a set of node embedding $\{\pmb{z}_1,...,\pmb{z}_{|\mathcal{V}|}\}$ to an embedding $\pmb{z}_{\mathcal{G}}$ that represents the full graph.

在5.2.2节讨论的 learning over sets of neighbor embeddings 方法  can also be employed for pooling at the graph level.

两种常用方法 or learning graph-level embeddings via set pooling. 

1. The first approach is simply to take a sum (or mean) of the node embeddings:
   $$
   \pmb{z}_{\mathcal{G}} = \frac{\sum_{v \in \mathcal{V}}\pmb{z}_v}{f_n(|\mathcal{V}|)}
   $$
   where $f_n$ is some normalizing function

2.  Using a combination of LSTMs and attention to pool the node embeddings (P64).  we iterate a series of attention based aggregations defined by the following set of equations, which are iterated for $t = 1, ..., T$  steps:
   $$
   \pmb{q}_t  = LSTM(\pmb{o}_{t-1},\pmb{q}_{t-1}) \\
   e_{v,t} = f_a(\pmb{z}_v,\pmb{q}_t), \forall v \in \mathcal{V} \\
   a_{v,t} = \frac{\exp(e_{v,i})}{\sum_{u \in \mathcal{V}} \exp(e_{u,t})} ,\forall v \in \mathcal{V} \\
   \pmb{o}_t =  \sum_{v \in \mathcal{V}} a_{v,t}\pmb{z}_v
   $$
   $\pmb{q}_t$ vector represents a **query vector** for the attention at each iteration $t$.  The query vector is used to compute an attention score over each node using an attention function $f_a:\mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$. 通常$\pmb{q}_0,\pmb{o}_0$ vectors are initialized with all-zero values, and after iterating for $T$ iterations, an embedding for the full graph is computed as
   $$
   \pmb{z}_{\mathcal{G}} = \pmb{o}_1 \oplus \pmb{o}_2 \oplus ... \oplus \pmb{o}_T
   $$



<font color=Aqua >**Graph coarsening approaches**</font>

One limitation of the set pooling approaches is that they do not exploit the structure of the graph.

虽然合理地将graph pooling的任务视为简单的集合学习问题，但在pooling阶段利用图拓扑也可以得到好处。One popular strategy to accomplish this is to perform graph **clustering or coarsening** as a means to pool the node representations.

In these style of approaches, we assume that we have some clustering function
$$
\pmb{f}_c \rightarrow \mathcal{G} \times \mathbb{R}^{|\mathcal{V}| \times d} \rightarrow \mathcal{R}^{+|\mathcal{V}| \times c}
$$
which maps all the nodes in the graph to an assignment over $c$ clusters. 特别我们假设这个函数输出一个赋值矩阵$\pmb{S} = f_c(\mathcal{G},\pmb{Z})$, where $\pmb{S}[u,i] \in \mathbb{R}^+$denotes the strength of the association between node $u$ and cluster $i$. 一个关于$f_c$的简单例子是第一章介绍的spectral clustering approach, where the cluster assignment is based on the spectral decomposition of the graph adjacency matrix. 另一种$f_c$参见(P65)

The key idea of graph coarsening approaches is that we then use this matrix to **coarsen** the graph.  In particular, we use the assignment matrix $\pmb{S}$ to compute a new coarsened adiacency matrix
$$
\pmb{A}^{new} = \pmb{S}^T\pmb{AS} \in \mathbb{R}^{+c \times c}
$$
and a new set of node features
$$
\pmb{X}^{new} = \pmb{S}^T \pmb{X} \in \mathbb{R}^{c \times d}
$$
Thus, this new adjacency matrix now represents the strength of association between the clusters in the graph, and the new feature matrix represents the aggregated embeddings for all the nodes assigned to each cluster. 我们可以在这个粗化的图上运行GNN，并重复整个粗化过程进行多次迭代，每一步图的大小都会减小。然后，通过在充分粗化的图中的节点嵌入上的集合池来计算图的最终表示。

该方法受到CNN的启发，并且它依赖于直觉，即我们可以构建在输入图的不同粒度上运行的分层GNN。实际中这种方法能够得到不错的效果，也可以导致不稳定与难以训练。例如要求为了端对端可微$f_c$必须可微，这样排除了很多现成的聚类算法。还有一些方法通过选择一组要删除的节点而不是将所有节点合并到集群中来粗化图形，这可以在计算复杂性和速度方面带来好处。(P66)



## 5.6 Generalized Message Passing

GNN消息传递方法也可以推广到在消息传递的每个阶段利用边缘和图形级别的信息。(P66)  We define each iteration of message passing according to the following equations: 
$$
\pmb{h}_{(u,v)}^{(k)}  = UPDATE_{edge} (\pmb{h}_{(u,v)}^{(k-1)},\pmb{h}_{u}^{(k-1)}, \pmb{h}_v^{(k-1)},\pmb{h}_{\mathcal{G}}^{(k-1)}) \\
\pmb{m}_{\mathcal{N}(u)} = AGGREGATE_{node} (\{\pmb{h}_{(u,v)}^{(k)},\forall v \in \mathcal{N}(u)\}) \\
\pmb{h}_u^{(k)}  = UPDATE_{node} (\pmb{h}_u^{(k-1)},\pmb{m}_{\mathcal{N}(u)},\pmb{h}_\mathcal{G}^{(k-1)})  \\
\pmb{h}_\mathcal{G}^{(k)} = UPDATE_{graph} (\pmb{h}_{\mathcal{G}}^{(k-1)},\{\pmb{h}_u^{(k)}, \forall u \in \mathcal{V}\},\{\pmb{h}_{(u,v)}^{(k)},\forall (u,v) \in  \mathcal{E}\})
$$
Recent work has also shown this generalized message passing approach to have benefifits compared to a standard GNN in terms of logical expressiveness (P66) 在消息传递期间为边和整个图生成嵌入也使得基于图或边级分类任务定义损失函数变得简单。

在这样一个通用消息传递框架中，所有单独的更新和聚合操作都可以使用本章讨论的技术来实现。





# 6.Graph Nerual Networks in Practice

上一章主要讨论了GNN的各种architectures. 但并没有讨论这些architectures如何优化和使用什么损失函数与正则函数。本章也会讨论无监督预训练方法，也会介绍正则与提升GNN效率的技术



## 6.1 Applications and Loss Functions

GNNs are used for one of three tasks: node classfication, graph classification, relation prediction. 本章主要讨论三种问题的损失函数，以及GNNs的预训练。



$\pmb{z}_u \in \mathbb{R}^d$ denote the node embedding output. $\pmb{z}_{\mathcal{G}} \in \mathbb{R}^d$ denote a graph-level embedding output by a pooling function. 第5章中讨论的任何GNN方法都可以用于生成这些嵌入。 In general, we will define loss functions on the $\pmb{z}_{\mathcal{G}}$ and $\pmb{z}_{u}$ embeddings.



### 6.1.1 GNNs for Node Classification

使用在论文分类(P69)

The standard way to apply GNNs to such a node classification task is to train GNNs in a fully-supervised manner, where we define the loss using a softmax classification function and negative log-likelihood loss:
$$
\mathcal{L} = \sum_{u \in \mathcal{V}_{train}} - \log(softmax(\pmb{z}_u,\pmb{y}_u))
$$
$\pmb{y}_u \in \mathbb{Z}^c$ is a one-hot vector indicating the class of training node $u \in \mathcal{V}_{train}$, 其中有；
$$
softmax(\pmb{z}_u,\pmb{y}_u) = \sum_{i=1}^c \pmb{y}_u[i] \frac{e^{\pmb{z}_u^T \pmb{w}_i}}{\sum_{j=1}^c e^{\pmb{z}_u^T \pmb{w}_j}}
$$
where $\pmb{w}_i \in \mathbb{R}^d,i=1,...,c$ are trainable parameters. 也有其他的监督损失函数，但公式(39)的是最常用的.

> **Supervised, semi-supervised, transductive, and inductive** we can distinguish between three types
>
> of nodes:
>
> 1. **Training nodes**, $\mathcal{V}_{train}$, These nodes are included in the GNN message passing operations, and they are also used to compute the loss.
> 2. **Transductive test nodes**, $\mathcal{V}_{trans}$. These nodes are unlabeled and not used in the loss computation, but these nodes—and their incident edges—are still involved in the GNN message passing operations. In other words, the GNN will generate hidden representations $\pmb{h}_u^{(k)}$ for the nodes in $u \in \mathcal{V}_{trans}$  during the GNN message passing operations. However, the final layer embeddings $\pmb{z}_u $ for these nodes will not be used in the loss function computation.
> 3. **Inductive test nodes**, $\mathcal{V}_{ind}$ These nodes are not used in either the loss computation or the GNN message passing operations during training, meaning that these nodes—and all of their edges—are completely unobserved while the GNN is trained.
>
> 半监督可以使用在transductive test nodes.The term inductive node classification is used to distinguish the setting where the test nodes—and all their incident edges—are completely unobserved during training. <font color=Red >**没懂incident node到底干啥用**</font>





### 6.1.2 GNNs for Graph Classification

 the key difference that the loss is computed with graph-level embeddings $\pmb{z}_{\mathcal{G}_i}$ over a set of labeled training graphs $\mathcal{T}  = \{\mathcal{G}_1,...,\mathcal{G}_n\}$. , It is standard to employ a squared-error loss of the following form:
$$
\mathcal{L} = \sum_{\mathcal{G}_i \in \mathcal{T}} \| MLP(\pmb{z}_{\mathcal{G}_i})  - y_{\mathcal{G}_i} \|_2^2
$$
where MLP is a densely connected neural network with a univariate output 



### 6.1.3 GNNs for Relation Prediction

使用到推荐系统与知识图谱上(P70) In these applications, the standard practice is to employ the pairwise node embedding loss functions introduced in Chapters 3 and 4.

In principle, GNNs can be combined with any of the pairwise loss functions discussed in those chapters, with the output of the GNNs replacing the shallow embeddings.



### 6.1.4 Pre-training GNNs

预训练文章(P71), 图预训练的例子: one could pre-train a GNN to reconstruct missing edges in the graph before fine-tuning on a node classification loss.

但预训练效果很小，甚至 a *randomly initialized GNN* is equally strong compared to one pre-trained on a neighborhood reconstruction loss. 解释这一发现的一个假设是，传递的GNN消息已经有效地编码了邻域信息。由于消息传递的结构，图中的相邻节点将倾向于在GNN中具有类似的嵌入，因此强制执行邻域重建损失可能只是冗余的。

也有比较好效果的预训练策略(P71)，**Deep Graph Infomax (DGI)**. which involves maximizing the mutual information between node embeddings $\pmb{z}_u$ and graph embedding $\pmb{z}_{\mathcal{G}}$. This approach optimizes the following loss:
$$
\mathcal{L} = -\sum_{u \in \mathcal{V}_{train}} \mathbb{E}_\mathcal{G} \log(D(\pmb{z}_u,\pmb{z}_\mathcal{G})) + \gamma \mathbb{E}_{\tilde{\mathcal{G}}} \log (1-D(\tilde{\pmb{z}}_u,\pmb{z}_\mathcal{G}))
$$
$\tilde{\pmb{z}}_u$ denotes an embedding of node $u$ generated based on a **corrupted** version of graph $\mathcal{G}$ denote $\tilde{\mathcal{G}}$. Using $D$ to denote a **discriminator function** which is a neural network trained to predict whether the node embedding came from the real graph $\mathcal{G}$ or the corrupted version $\tilde{\mathcal{G}}$. 通常，通过在node features, adjacency matrix, or both in some stochastic manner(e.g. shuffling entries of the feature matrix)作出改变来获得corrupted graph. 

The intuition behind this loss is that the GNN model must learn to generate node embeddings that can distinguish between the real graph and its corrupted counterpart. 

The loss function used in DGI (Equation (42)) is just one example of a broader class of unsupervised objectives that have witnessed success in the context of GNNs(P71)

These unsupervised training strategies generally involve training GNNs to maximize the mutual information between different levels of representations or to distinguish between real and corrupted pairs of embeddings.



## 6.2  Efficiency Concerns and Node Sampling

对于第五章的node-level message passing equations,直接使用会使得计算效率低下



### 6.2.1  Graph-level Implementations

为了减少message passing的计算，最有效的方法是使用graph-level的GNN公式

The key idea is to implement the message passing operations based on sparse matrix multiplications.例如the graph-level equation for a basic GNN is given by
$$
\pmb{H}^{(k)} = \sigma \Big(\pmb{AH}^{(k-1)} \pmb{W}_{neigh}^{(k)} + \pmb{H}^{(k-1)}\pmb{W}_{self}^{(k)}\Big)
$$
where $\pmb{H}^{(k)}$ is a matrix containing the layer-$k$ embeddings of all the nodes in the graph.使用这个公式的优点在于不需要冗余的计算，i.e., we compute the embedding $\pmb{h}_u^{(k)}$ for each node $u$ exactly once when running the model. 这种方法的局限是，需要在整个图和所有点特征上同时操作，可能会需要巨大的存储，此外，使用图级方程本质上限制了一到全批次（与小批量）梯度下降。



### 6.2.2 Subsampling and Mini-Batching

In order to limit the memory footprint of a GNN and facilitate mini-batch training, one can work with a subset of nodes during message passing.

The challenge, however, is that we cannot simply run message passing on a subset of the nodes in a graph without losing information.每次移除一个node也会丢失他的边。

One strategy to overcome this issue by subsampling node neighborhoods.(P73相关的三篇论文)



## 6.3 Parameter Sharing and Regularization



L2 regularization, dropout and layer normalization.(P73) There are also regularization strategies that are somewhat specific to the GNN setting.

<font color=Aqua >**Parameter Sharing Across Layers**</font>

The core idea is to use the same parameters in all the AGGREGATE and UPDATE functions in the GNN.这种方法在超过6层的时候非常有效并且经常与gated update functions相结合.(P73)

<font color=Aqua >**Edge Dropout**</font>

Randomly remove (or mask) edges in the adjacency matrix during training, with the intuition that this will make the GNN less prone to over-fitting and more robust to noise in the adjacency matrix.(P73)

It was an essential technique used in the original graph attention network (GAT) work. (P73)





# 7. Theoretical Motivations





一方面，GNNs were developed based on the theory of graph signal processing, as a generalization of Euclidean convolutions to the non-Euclidean graph domain (P75)

同时 neural message passing approaches—which form the basis of most modern GNNs—were proposed by analogy to message passing algorithms for probabilistic inference in graphical models (P75)

最后 GNNs have been motivated in several works based on their connection to the Weisfeiler-Lehman graph isomorphism test(P75)



## 7.1 GNNs and Graph Convolutions



### 7.1.1 Convolutions and the Fourier Transform

Let $f$ and $h$ be two functions. We can define the general continuous convolution operation $\star$ as 
$$
(f \star h)(\pmb{x}) = \int_{\mathbb{R}^d} f(\pmb{y}) h(\pmb{x-y}) d\pmb{y}
$$
Convolution operation can be computed by an element-wise product of the **Fourier transforms** of the two functions:
$$
(f \star h)(\pmb{x}) = \mathcal{F}^{-1} (\mathcal{F}(f(\pmb{x})) \circ \mathcal{F}(h(\pmb{x})))
$$
where
$$
\mathcal{F}(f(\pmb{x})) = \hat{f}(\pmb{s}) = \int_{\mathbb{R}^d} f(\pmb{x}) e^{-2\pi \pmb{x}^T \pmb{s}i} d \pmb{x}
$$
is the Fourier transform of $f(\pmb{x})$ and its inverse Fourier transform is defined as
$$
\mathcal{F}^{-1} (\hat{f}(\pmb{s})) = \int_{\mathbb{R}^d} \hat{f}(\pmb{s})e^{2\pi \pmb{s}^T \pmb{s}i} d\pmb{s}
$$
In the simple case of univariate discrete data over a finite domain $t \in \{0,...,N-1\}$ we can simplify these operations to a discrete circular convolution
$$
(f \star_N h) (t) = \sum_{\tau =0}^{N-1} f(\tau)h((t-\tau)_{mod \ N})
$$
and a discrete Fourier transform(DFT)`mod`是[数学模函数](https://en.wikipedia.org/wiki/Modulo_operation)（将第一个参数除以第二个参数后的余数）
$$
s_k = \frac{1}{\sqrt{N}} \sum_{t=0}^{N-1} f(x_t)e^{-\frac{i2\pi}{N}kt}= \frac{1}{\sqrt{N}} \sum_{t=0}^{N-1} f(x_t) \Bigg(\cos\Big(\frac{2\pi}{N}kt\Big)-i\sin \Big(\frac{2\pi}{N}kt\Big)\Bigg)
$$
<font color=Red >**$s_k$是啥没出现过啊**</font>where $s_k \in \{s_0,..,s_{N-1}\}$ is the Fourier coefficient corresponding to the sequence $(f(x_0),f(x_1),...,f(x_{N-1}))$. 在公式(48)中，使用$\star_{N}$ to emphasize that this is a circular convolution defined over the finite domain $\{0,...,N-1\}$, but we will often omit this subscript for notational simplicity.

> **Interpreting the (discrete) Fourier transform** The Fourier transform essentially tells us how to represent our input signal as a weighted sum of (complex) sinusoidal waves. 如果我们假定输入数据与其 Fourier transform 均为实值. we can interpret the sequence $[s_0,...,s_{N-1}]$ as the coefficients of a Fourier series. 这种视角下,$s_k$ tells us amplitude of the complex sinusoidal component $e^{-\frac{i 2\pi}{N}k}$, which has frequency $\frac{2\pi k}{N}$.

In terms of signal processing, we can view the discrete convolution $f \star h$ as a **filtering operation** of the series $(f(x_1),f(x_2),...,f(x_N))$ by a filter $h$. 

Generally, we view the series as corresponding to the values of the signal throughout time, and the convolution operator applies some filter (e.g., a band-pass filter) to modulate this time-varying signal.

One critical property of convolutions, which we will rely on below, is the fact that they are **translation (or shift) equivariant:**
$$
f(t+a) \star g(t) = f(t) \star g(t+a) = (f \star g)(t+a)
$$
This property means that translating a signal and then convolving it by a filter is equivalent to convolving the signal and then translating the result.<font color=Red >**??**</font>

Note that as a corollary convolutions are also equivariant to the difference operation:
$$
\Delta f(t) \star g(t)  = f(t) \star \Delta g(t) = \Delta (f \star g)(t)
$$
where 
$$
\Delta f(t) = f(t+1) - f(t)
$$
is the Laplace (i.e., difference) operator on discrete univariate signals.

关于这个部分的相关书籍(P77)<font color=Red >**这节看不懂**</font>



### 7.1.2 From Time Signal to Graph Signals

We now discuss how we can connect discrete time-varying signals with signals on a graph. Suppose we have a discrete time-varying signal $f(t_0),f(t_1),...f(t_{N-1})$

<div align=center>
<img src="/Users/guass/Desktop/markdown/图神经网络/Graph Representation Learning/5png2.png" width="600" height=""/>
</div>

One way of viewing this signal is as corresponding to a chain (or cycle) graph. 其中每个时间点$t$表示为一个节点，每个函数值$f(t)$表示该时间/节点的信号值。从这个角度来看，将信号表示为向量$\pmb{f} \in \mathbb{R}^N$​是很方便的，每个维度对应于链图中的不同节点。图中的边表示信号如何传播。Note that we add a connection between the last and first nodes in the chain as a boundary condition to keep the domain finite.

In particular, the adjacency matrix for this chain graph corresponds to the circulant matrix $\pmb{A}_c$ with
$$
\pmb{A}_c[i,j] = \begin{cases} 1 \text{ if } j = (i+1)_{mod \ N} \\ 0 \ \ \text{  otherwise,} \end{cases}
$$
and the (unnormalized) Laplacian $\pmb{L}_c$ for this graph can be defined as
$$
\pmb{L}_c = \pmb{I -A}_c
$$
We can then represent time shifts as multiplications by the adjacency matrix,
$$
(\pmb{A}_c \pmb{f})[t] = \pmb{f}[(t+1)_{mod\ N}]
$$
and the difference operation by multiplication by the Laplacian,
$$
(\pmb{L}_c \pmb{f}) [t] = \pmb{f}[t] - \pmb{f}[(t+1)_{mod \ N}]
$$
通过这种方式，我们可以看到图的邻接矩阵和拉普拉斯矩阵，以及信号的移位和差分概念之间有着密切的联系。将信号与邻接矩阵相乘可将信号从一个节点传播到另一个节点，通过拉普拉斯算子相乘可计算每个节点上的信号与其相邻节点之间的差值。

我们可以类似地将滤波器$h$的卷积表示为向量$\pmb{f}$上的矩阵多重乘法
$$
(f \star h)(\pmb{t}) = \sum_{\tau = 0}^{N-1} f(\tau) h(\tau - t) = \pmb{Q}_h \pmb{f}
$$

where $\pmb{Q}_{h} \in \mathbb{R}^{N \times N}$ is a matrix representation of the convolution operation by filter function $h$ and $\pmb{f} = [f(t_0),f(t_2),...,f(t_{N-1})]^T$ is a vector representation of the funtion $f$。 为了让上面第二个等式成立，$\pmb{Q}_h$必须满足一些特殊的性质。特别地，我们要求该矩阵的乘法满足平移等价，这对应于与循环邻接矩阵(circulant adjacency)$\pmb{A}_c$的交换性(commutativity)。也就是满足
$$
\pmb{A}_c \pmb{Q}_h = \pmb{Q}_h \pmb{A}_c \Rightarrow \pmb{L}_c \pmb{Q}_h = \pmb{Q}_h \pmb{L}_c
$$
It can be shown that these requirements are satisfied for a real matrix $\pmb{Q}_h$ if
$$
\pmb{Q}_h = p_N(\pmb{A}_c) = \sum_{i=0}^{N-1} \alpha_i \pmb{A}_c^i
$$
**Generalizing to General Graphs**

对于任意图对应的邻接矩阵就$\pmb{A}$, 可以将卷积滤波器表示为以下形式的矩阵
$$
\pmb{Q}_h = \alpha_0 \pmb{I} + \alpha_1\pmb{A} + \alpha_2\pmb{A}^2+ ...\alpha_N \pmb{A}^N
$$
Intuitively, this gives us a *spatial* construction of a convolutional filter on graphs.

乘node feature vector $\pmb{x} \in \mathbb{R}^{|V|}$ 有
$$
\pmb{Q}_h  \pmb{x}= \alpha_0 \pmb{I}\pmb{x} + \alpha_1\pmb{A}\pmb{x} + \alpha_2\pmb{A}^2\pmb{x}+ ...\alpha_N \pmb{A}^N\pmb{x}
$$
这意味着每个节点$u \in \mathcal{V}$的卷积信号 $\pmb{Q}_{h} \pmb{x}[u]$ 将会对应一些N-hop近邻的节点信息。

若是高维节点信息$\pmb{X} \in \mathbb{R}^{|\mathcal{V}| \times m}$ 则有
$$
\pmb{Q}_h  \pmb{X}= \alpha_0 \pmb{I}\pmb{X} + \alpha_1\pmb{A}\pmb{X} + \alpha_2\pmb{A}^2\pmb{X}+ ...\alpha_N \pmb{A}^N\pmb{X}
$$
**Graph Convolutions and Message Passing GNNs**

一些关于消息传递与卷积关系的描述

> **adjacency matrix, Laplacian, or a normalized variant？**
>
> 上述介绍的$\pmb{Q}_h$满足filter commutes with the adjacency matrix，该性质称为**translation equivariance**. 但普通的邻接矩阵的拉普拉斯矩阵$\pmb{L=D-A}$​并不一定满足commutativity. For more general graphs we have a choice to make in terms of whether we define convolutions based on the adjacency matrix or some version of the Laplacian. 
>
> 实际中经常使用symmetric normalized Laplacian $\pmb{L}_{sym} = \pmb{D}^{-1/2}\pmb{LD}^{-1/2}$ 或者 symmetric normalized adjacency matrix$\pmb{A}_{sym} = \pmb{D}^{-1/2}\pmb{AD}^{-1/2}$ 去定义卷积过滤器。两个原因使用这两个东西：
>
> - First, both these matrices have bounded spectrums, which gives them desirable numerical stability properties.
> - Second, —these two matrices are *simultaneously diagonalizable*,  which means that they share the same eigenvectors.
>
> This means that defining filters based on one of these matrices implies commutativity with the other, which is a very convenient and desirable property.



### 7.1.3 Spectral Graph Convolutions

one key property of convolutions that we ignored in the previous subsection is the relationship between convolutions and the Fourier transform.

**The Fourier transform and the Laplace operator**

To motivate the generalization of the Fourier transform to graphs, we rely on the connection between the Fourier transform and the Laplace (i.e., difference) operator.

之前讲的$\Delta$操作可以拓展到任意光滑函数$f:\mathbb{R}^d \rightarrow \mathbb{R}$
$$
\Delta f(\pmb{x}) = \nabla^2 f(\pmb{x}) - \sum_{i=1}^n \frac{\part^2 f}{\part x^2}
$$
Intuitively, the Laplace operator tells us the average difference between the function value at a point and function values in the neighboring regions surrounding this point.
$$
(\pmb{Lx})[i] = \sum_{j \in \mathcal{V}} \pmb{A}[i,j] (\pmb{x}[i]-\pmb{x}[j])
$$
通过这种方式，我们可以将拉普拉斯矩阵视为拉普拉斯算子的离散模拟，因为它允许我们量化某个节点上的值与该节点相邻节点上的值之间的差异。

Now, an extremely important property of the Laplace operator is that its eigenfunctions correspond to the complex exponentials. That is,
$$
-\Delta (e^{2\pi ist} ) = -\frac{\part^2 (e^{2\pi ist})}{\part t^2} = (2\pi s)^2 e^{2\pi ist}
$$
so the eigenfunctions of $\Delta$ are the same complex exponetials that make up the modes of the frequency domain in the Fourier transform, with the corresponding eigenvalue indicating the frequency.

In fact, one can even verify that the eigenvectors $\pmb{u}_1,...,\pmb{u}_n$ of the circulant Laplacian $\pmb{L}_c \in \mathbb{R}^{n \times n}$ for the chain graph are $\pmb{u}_j = \frac{1}{\sqrt{n}}[1,\omega_j,\omega_j^2,...,\omega_j^n]$ where $\omega_j = e^\frac{2\pi j}{n}$

**The graph Fourier transform**

拉普拉斯算子的本征函数与傅里叶变换之间的联系使我们能够将傅里叶变换推广到任意图。特别是，我们可以通过考虑一般图Laplacian的特征分解来推广傅里叶变换的概念：
$$
\pmb{L} = \pmb{U \Lambda U}^T
$$
其中，我们将特征向量$\pmb{U}$定义为*graph Fourier modes*，作为基于图形的傅里叶模式概念。The matrix $\pmb{\Lambda}$​ is assumed to have the corresponding eigenvalues along the diagonal, and these eigenvalues provide a graph-based notion of different frequency values.

由于一般拉普拉斯算子的特征函数对应于傅里叶模式，即傅里叶级数中的复指数，我们基于图拉普拉斯算子的特征向量定义了一般图的傅里叶模式。

Thus, the Fourier transform of signal (or function) $\pmb{f} \in \mathbb{R}^{|\mathcal{V}|}$ on a graph can be computed as
$$
\pmb{s = U^Tf}
$$
and its inverse Fourier transform computed as
$$
\pmb{f = Us}
$$
给定graph Fourier coefficients $\pmb{U}^T\pmb{f}$ of a signal $\pmb{f}$ as well as the graph Fourier coefficients $\pmb{U}^T\pmb{h}$ of some filter $\pmb{h}$, we can compute a graph convolution via element-wise products as
$$
\pmb{f} \star_\mathcal{G} \pmb{h} = \pmb{U}(\pmb{U}^T\pmb{f} \circ \pmb{U}^T \pmb{h})
$$
where $\pmb{U}$ is the matrix of eigenvectors of the Laplacian $\pmb{L}$ and $\star_\mathcal{G}$表示this convolution is specific to a graph $\mathcal{G}$

基于公式(69)，我们可以根据函数$h$的图傅里叶系数$\theta_h = \pmb{U}^T \pmb{h} \in \mathbb{R}^{|\mathcal{V}|}$在谱域中表示卷积。例如，我们可以通过直接优化$\theta_h$并将卷积定义为:
$$
\pmb{f} \star_\mathcal{G} \pmb{h} = \pmb{U}(\pmb{U}^T\pmb{f} \circ \theta_h) = (\pmb{U}diag(\theta_h)\pmb{U}^T)\pmb{f}
$$
然而，以这种非参数方式定义的过滤器对图的结构没有真正的依赖性，并且可能无法满足我们希望从卷积中获得的许多属性。例如，这种滤波器可以是任意的**非局部**滤波器。

为了确保谱滤波器$\theta_h$对应于图上有意义的卷积，自然的解决方案是基于拉普拉斯算子的特征值对$\theta_h$进行参数化。特别地，我们可以将谱滤波器定义为$p_N(\pmb{\Lambda})$，因此它是拉普拉斯特征值的$N$次多项式。以这种方式定义谱卷积可以确保我们的卷积与拉普拉斯变换一致，因为
$$
\pmb{f} \star_\mathcal{G} \pmb{h} = (\pmb{U}p_N(\pmb{\Lambda})\pmb{U}^T )\pmb{f} = p_N(\pmb{L})\pmb{f}
$$
此外，这一定义确保了局部性的概念。如果我们使用$k$次多项式，那么我们确保每个节点处的滤波信号取决于其跳$k$邻域中的信息。

因此，最后，从谱角度导出图卷积，我们可以恢复这样一个关键思想：图卷积可以表示为拉普拉斯多项式（或其规范化变体之一）。然而，谱的角度也揭示了定义图上卷积的更一般的策略。

> **Interpreting the Laplacian eigenvectors as frequencies** 在标准傅里叶变换中，我们可以将傅里叶系数解释为对应于不同的频率。在一般图的情况下，我们不能再以这种方式解释图的傅里叶变换。然而，我们仍然可以对高频和低频分量进行类比。特别是，我们可以回忆起拉普拉斯算子的特征向量$\pmb{u}_i,i=1,...,|\mathcal{V}|$解决了最小化问题：
> $$
> \min_{\pmb{u}_i \in \mathbb{R}^{|\mathcal{V}|}:\pmb{u}_i \perp \pmb{u}_j \forall j <i} \frac{\pmb{u}_i^T \pmb{L}\pmb{u}_i}{\pmb{u}_i^T \pmb{u}_j}
> $$
> by the Rayleigh-Ritz Theorem. And we have that
> $$
> \pmb{u}_i^T \pmb{Lu}_i  = \frac{1}{2} \sum_{u,v \in \mathcal{V}}\pmb{A}[u,v](\pmb{u}_i[u] - \pmb{u}_i[v])^2
> $$
> 通过第1章讨论的拉普拉斯算子的性质。这些事实共同表明，拉普拉斯函数的最小特征向量对应于图上每个节点变化量最小的信号，第二小特征向量对应于变化量第二小的信号，依此类推。事实上，我们在第1章进行谱聚类时利用了拉普拉斯特征向量的这些性质。在这种情况下，我们证明了拉普拉斯特征向量可用于将节点分配给社区，从而使社区之间的边数最小化。我们现在可以从信号处理的角度来解释这个结果：拉普拉斯特征向量定义了在整个图中以平滑方式变化的信号，最平滑的信号表示图的粗粒度社区结构。



### 7.1.4 Convolution-Inspired GNNs

前面的小节将卷积的概念推广到图。我们看到图上的基本卷积滤波器可以表示为（归一化）邻接矩阵或拉普拉斯多项式。我们看到了这一事实的空间和谱动机，我们看到了如何使用谱透视图来定义基于图形傅里叶变换的图形卷积的更一般形式。在本节中，我们将简要回顾如何基于这些联系开发和启发不同的GNN模型。

**Purely Convolutional Approaches**



关于GNNs的一些最早的工作可以直接映射到前面小节的图卷积定义。这些方法的关键思想是，它们使用方程（70）或方程（71）来定义卷积层，并通过叠加和组合具有非线性的多个卷积层来定义完整模型。比如有 non-parametric spectral filter，也有使用参数模型更改$p_N(\pmb{\Lambda}),p_N(\pmb{L})$ (P85有几篇论文) 也有更改拉普拉斯或邻接矩阵的实值多项式的，比如改成Cayley polynomials

**Graph convolutional networks and connections to message passing**

raph convolutional network (GCN). The key insight of the GCN approach is that we can build powerful models by stacking very simple graph convolutional layers. A basic GCN layer is defined as 
$$
\pmb{H}^{(k)} = \sigma \Big(\bar{A}\pmb{H}^{(k-1)}\pmb{W}^{(k)}\Big)
$$
where $\pmb{\bar{A}} = (\pmb{D}+\pmb{I})^{-\frac{1}{2}}(\pmb{A}+\pmb{I})(\pmb{D}+\pmb{I})^{-\frac{1}{2}}$ is a normalized variant of the adjacency matrix (with self-loops) and $\pmb{W}^{(k)}$ is a learnable parameter matrix. 该模型最初是由简单图卷积（基于多项式$\pmb{A}+\pmb{I}$），学习权重矩阵和非线性组合而成。如第5章所述，我们还可以将GCN模型解释为基本GNN消息传递方法的变体。一般而言，如果我们考虑通过多项式$\pmb{I+A}$定义的简单的图卷积与非线性和可训练权重矩阵相结合，则我们恢复基本GNN：
$$
\pmb{H}^{(k)} = \sigma \Big(\pmb{A}\pmb{H}^{(k-1)}\pmb{W}_{neigh}^{(k)} + \pmb{H}^{(k-1)}\pmb{W}_{self}^{(k)}\Big)
$$
公式(74)是一个基础班的GCN，(75)是一个简单的GCN与之前的GNN一样的东西



换句话说，基于$\pmb{I}+\pmb{A}$的简单图卷积等价于聚集来自邻居的信息并将其与来自节点本身的信息相结合。因此，我们可以将消息传递的概念视为与附加可训练权重和非线性相结合的图卷积的简单形式相对应。

> **Over-smoothing as a low-pass convolutional filter**:  Based on the connection between message-passing GNNs and graph convolutions, we can now understand over-smoothing from the perspective of graph signal processing.
>
> 关键的直觉是，在一个基本GNN中叠加多轮消息传递类似于应用低通卷积滤波器，它在图上生成输入信号的平滑版本。
>
>  特别，假定我们基本GNN(公式(75)) 简化为以下更新方程式：
>  $$
>  \pmb{H}^{(k)} = \pmb{A}_{sym} \pmb{H}^{(k-1)} \pmb{W}^{(k)}
>  $$
>
>  对比(75)，我们通过消除非线性和在每个消息传递步骤中添加“自”嵌入来简化模型。为了数学简化和数值稳定，我们还将假设我们使用的是对称标准化邻接矩阵$\pmb{A}_{sym} = \pmb{D}^{-1/2}\pmb{A}\pmb{D}^{-1/2}$而不是非标准的邻接矩阵。这个模型与简单GCN方法类似(P86)并且本质上相当于在每一轮消息传递中取相邻嵌入的平均值。
>
>  在$K$轮消息传递之后，基于公式(76)，我们将得到一个表示，它取决于邻接矩阵的第$K$次幂：
>  $$
>  \pmb{H}^{(K)} = \pmb{A}_{sym}^K \pmb{XW}
>  $$
>  其中$\pmb{W}$是一些线性算子，$\pmb{X}$是输入节点特征的矩阵。为了理解过平滑滤波器和卷积滤波器之间的联系，我们只需要认识到，输入节点特征与邻接矩阵的高次幂的乘积$\pmb{A}_{sym}^K \pmb{X}$可以解释为基于图拉普拉斯算子的最低频率信号的卷积滤波器。
>
>  例如，假设我们使用了足够大的$K$值，这样我们就达到了以下循环的固定点：
>  $$
>  \pmb{A}_{sym} \pmb{H}^{(K)} = \pmb{H}^{(K)}
>  $$
>  当使用规范化邻接矩阵时，可以验证该不动点是可实现的，因为$\pmb{A}_{sym}$的主导特征值等于1。
>
>  可见在固定点，所有节点特征都将收敛到完全由$\pmb{A}_{sym}$的主导特征向量定义，更一般地说，$\pmb{A}_{sym}$的高次幂会强调这个矩阵的最大特征值。此外，我们知道$\pmb{A}_{sym}$的最大特征值对应于其对应物对称规范化拉普拉斯$\pmb{L}_{sym}$的最小特征值($\pmb{L}_{sym} = \pmb{U\Lambda U}^T \quad \pmb{A}_{sym} = \pmb{U}(\pmb{I-\Lambda})\pmb{U}^T$)总之，这些事实表明，将信号乘以$\pmb{A}_{sym}$的高幂对应于基于$\pmb{L}_{sym}$的最低特征值（或频率）的卷积滤波器，即，它产生*低*通滤波器！
>
>  因此，我们可以从这个简化模型中看出，叠加多轮消息传递会导致低通卷积滤波器，在最坏的情况下，这些滤波器只是将所有节点表示收敛到图上连接成分（这个应该就是说那些连接到点）的常量值（即拉普拉斯函数的“零频率”）。



**GNNs without message passing**

受图卷积连接的启发，最近的几项工作也提出通过删除迭代消息传递过程来简化GNN。在这些方法中，模型通常定义为
$$
\pmb{Z} = MLP_{\theta}(f(\pmb{A})MLP_{\phi}(\pmb{X}))
$$
其中$f:\mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|} \rightarrow \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|}$ 是邻接矩阵$\pmb{A}$的确定函数，MLP表示密集神经网络，$\pmb{X} \in \mathbb{R}^{|\mathcal{V} \|\times m}$为输入节点特征，$\pmb{Z} \in \mathbb{R}^{|\mathcal{V}| \times d}$ 是可以学习的节点表示矩阵。例如可以定义
$$
f(\pmb{A}) = \pmb{\bar{A}}^k
$$
其中$\pmb{\bar{A}} = (\pmb{D}+\pmb{I})^{-\frac{1}{2}} (\pmb{A}+\pmb{I}) (\pmb{D}+\pmb{I})^{-\frac{1}{2}}$为对称标准化邻接矩阵。在一项密切相关的工作中，Klicpera等人[2019]通过类比个性化PageRank算法将$f$定义为
$$
f(\pmb{A}) = \alpha (\pmb{I}-(1-\alpha)\pmb{\bar{A}})^{-1} = \alpha \sum_{k=0}^\infty (\pmb{I} - \alpha \bar{A})^k
$$
注意：公式(81)等式成立的条件是$(\pmb{I}- \alpha \pmb{A})$的主导特征值以1为界

这些方法背后的直觉是，我们通常不需要将可训练神经网络与图卷积层交错。相反，我们可以简单地使用神经网络来学习模型开始和结束时的特征转换，并应用确定性卷积层来利用图形结构。在许多分类基准上，这些简单模型的性能优于参数化程度更高的消息传递模型（如GATs或GraphSAGE）。

还有越来越多的证据表明，使用具有自循环的对称规范化邻接矩阵可以产生有效的图卷积，特别是在这种没有消息传递的简化设置中。已经有结果并且被证明$\pmb{\bar{A}}$可以达到最好的经验表现。增加自环可以通过降低主导特征值的大小来缩小相应图的拉普拉斯谱。直观地说，添加自循环可以减少远处节点的影响，并使过滤后的信号更依赖于图上的局部邻域。





## 7.2 GNNs and Probabilistic Graphical Models

GNN作为卷积对图形结构化数据的扩展，得到了很好的理解和激励。然而，GNN框架还有其他理论动机，可以提供有趣和新颖的视角。一个突出的例子是基于概率图形模型（PGM）中变分推理连接的GNN动机。

在概率角度，我们看到的是，$\pmb{z}_u,\forall u \in \mathcal{V}$作为我们试图推断的潜在变量(latent variable)。我们假设我们观察图形结构（即邻接矩阵$\pmb{A}$）和输入节点特征$\pmb{X}$，我们的目标是推断潜在变量(即嵌入$\pmb{z}_v$​)。这可以解释观察到的数据。然后，作为GNNs基础的消息传递操作可以被视为某些消息传递算法的神经网络模拟，这些算法通常用于变分推理，以推断潜在变量的分布。(P88)

关于PGMs可看(P88)

### 7.2.1 Hilbert Space Embeddings of Distributions

首先介绍希尔伯特空间：$p(\pmb{x})$ 表示概率密度函数定义在r.v.s $\pmb{x} \in \mathbb{R}^m$上。给定任意一个特征映射(可能是有限维)$\phi : \mathbb{R}^m \rightarrow \mathcal{R}$，我们可以根据此特征映射下的期望值表示密度$p(\pmb{x})$：
$$
\mu_{\pmb{x}} = \int_{\mathbb{R}^m} \phi(\pmb{x})p(\pmb{x})d \pmb{x}
$$


Hilbert空间嵌入分布的关键思想是，只要使用合适的特征映射$\phi$，公式(82)将是内射的(injective)。这意味着$\mu_{\pmb{x}}$可以作为$p(\pmb{x})$的充分统计信息，我们想要在$p(\pmb{x})$上执行的任何计算都可以等价地表示为嵌入$\mu_{\pmb{x}}$的函数。保证这种内射性质的特征映射的一个众所周知的例子是由高斯径向基函数（RBF）核诱导的特征映射

分布的Hilbert空间嵌入研究是统计学的一个丰富领域。然而，在连接到GNNs的上下文中，关键在于我们可以将分布$p(\pmb{x})$表示为某些特征空间中的嵌入$\mu_{\pmb{x}}$。我们将使用这个概念来激励GNN消息传递算法，作为学习表示节点延迟$p(\pmb{z}_v)$上分布的嵌入的一种方式。



### 7.2.2 Graphs as Graphical Models

从图数据的概率角度来看，我们可以假设给定的图结构定义了不同节点之间的依赖(dependencies)关系。当然，我们通常用这种方式解释图形数据。通常假设图中连接的节点以某种方式相关。然而，在概率设置中，我们以一种正式的概率方式看待节点之间依赖的概念。

对于一个图$\mathcal{G} = (\mathcal{V},\mathcal{E})$定义了一个马尔科夫随机场(Markov random field)：


$$
p(\{\pmb{x}_v\},\{\pmb{z}_v\}) \propto \prod_{v \in V} \Phi(\pmb{x}_v,\pmb{z}_v) \prod_{(u,v) \in \mathcal{E}} \Psi (\pmb{z}_u,\pmb{z}_v)
$$
其中$\Phi,\Psi$是非负potential functions，这里$\{\pmb{x}_v\}$是$\{\pmb{x}_v,\forall v \in \mathcal{V}\}$的缩写。公式(83)表示，节点特征和节点嵌入上的分布$p(\{\pmb{x}_v\},\{\pmb{z}_v\})$根据图结构分解。直观来说$\Phi(\pmb{x}_v,\pmb{z}_v)$表示给定隐节点嵌入$\pmb{z}_v$下的特征向量$\pmb{x}_v$的似然函数，而$\Psi$控制着连接节点间的依赖性。我们因此假定节点特征被他们的隐嵌入决定并且假定相邻接点的隐嵌入相互依赖。

在标准的概率模型中，$\Psi,\Phi$通常定义为一个基于领域知识的参数函数，而且，通常假设这些函数来自指数族，以确保可处理性(P89)。然而，我们不知道$\Psi,\Phi$的确切形式，我们将通过利用上一节讨论的希尔伯特空间嵌入思想来隐式学习这些函数。



### 7.2.3 Embedding mean-filed inference

给定由公式(83)定义的马尔可夫随机场，我们的目标是推断出所有节点$v \in \mathcal{V}$的隐嵌入分布$p(\pmb{z}_v)$同时学习函数$\Psi,\Phi$。更直观地说，我们的目标是推断图中所有节点的潜在表示，以解释观察到的节点特征之间的依赖关系。

为了做到这一点，一个关键步骤是计算后验$p(\{\pmb{x}_v\},\{\pmb{z}_v\})$，即计算给定观察特征的一组特定潜在嵌入的可能性。一般来说，即使$\Psi,\Phi$已知且定义良好，计算该后验值在计算上也是困难的，因此我们必须求助于近似方法。

我们将在这里利用的一种流行方法是采用平均场变分推理(mean-field variational inference)，其中我们基于以下假设，使用一些函数$q_v$近似后验值：
$$
p(\{\pmb{z}_v\}|\{\pmb{x}_v\}) \approx q(\{\pmb{z}_v\}) = \prod_{v \in \mathcal{V}} q_v(\pmb{z}_v)
$$
其中每个$q_v$是密度函数。平均场推断的关键直觉是，我们假设潜在变量的后验分布分解为$\mathcal{V}$独立分布，每个节点一个。

为了获得在平均场近似中最优的近似$q_v$函数，标准方法是最小化近似后验值和真实后验值之间的Kullback–Leibler（KL）散度：
$$
KL(q(\{\pmb{z}_v\})|p(\{\pmb{z}_v\}| \{\pmb{x}_v\})) = \int_{(\mathbb{R}^d)^\mathcal{V}} \prod_{v \in \mathcal{V}} q(\{\pmb{z}_v\}) \log \Big( \frac{\prod_{v \in \mathcal{V}}q(\{\pmb{z}_v\})}{p(\{\pmb{z}_v\}| \{\pmb{x}_v\})}\Big) \prod_{v \in \mathcal{V}} d \pmb{z}_v
$$
KL散度是测量概率分布之间距离的一种标准方法，因此，在平均场假设下，找到使公式(85)最小化的$q_v$函数可以得到尽可能接近真实后验值的近似后验值。当然，直接最小化公式(85)是不可能的，因为评估KL散度需要了解真实后验概率。

然而，幸运的是，变分推理技术可以用来证明最小化KL的$q_v(\pmb{z}_v)$必须满足以下不动点方程：
$$
\log(q(\pmb{z}_v)) = c_v + \log(\Phi(\pmb{x}_v,\pmb{z}_v)) + \sum_{u \in \mathcal{N}(v)} \int_{\mathbb{R}^d} q_u(\pmb{z}_u) \log (\Psi(\pmb{z}_u,\pmb{z}_v)) d\pmb{z}_u
$$
其中$c_v$是一个常数并且不依赖于$q_v(\pmb{z}_v)$或$\pmb{z}_v$。在实际应用中，我们可以通过将一些初始猜测$q_v^{(t)}$初始化为有效的概率分布并迭代计算来近似这个不动点解
$$
\log \Big( q_{v}^{(t)}(\pmb{z}_v) \Big) = c_v+\log(\Phi(\pmb{x}_v,\pmb{z}_v))+\sum_{u \in \mathcal{N}(v)} \int_{\mathbb{R}^d} q_u^{(t-1)} (\pmb{z}_u) \log (\Psi(\pmb{z}_u,\pmb{z}_v ))d \pmb{z}_u
$$
公式(86)的必要的思想为:

1. 我们能够在隐嵌入上使用mean-filed假设来近似真实后验分布$p(\{\pmb{z}_v\}|\{\pmb{x}_v\}) $，其中我们假设后验因子分解为$|\mathcal{V}|$独立分布$p(\{\pmb{z}_v\}|\{\pmb{x}_v\}) \approx \prod_{v \in \mathcal{V}}q_v(\pmb{z}_v)$。

2. 平均场假设下的最佳近似由公式(86)中的不动点给出，其中每个潜在节点嵌入的近似后验$q_v(\pmb{z}_v)$是(i)节点特征$\pmb{z}_x$和(ii)节点相邻嵌入的边缘分布$q_u(\pmb{z})_u,\forall u \in \mathcal{N}(v)$的函数。

此时，与GNNs的连接开始出现。特别是，如果我们检查方程(87)中的不动点迭代，我们会发现更新后的边缘分布$ q_{v}^{(t)}(\pmb{z}_v)$是节点特征$\pmb{x}_v$（通过势函数$\Phi$）的函数，，以及前一次迭代中相邻边缘集$\{q_u^{(t-1)}(\pmb{z}_u),\forall u \in \mathcal{N}(v)\}$（通过势函数$\Psi$）的函数。这种形式的消息传递与GNNs中的消息传递非常相似！在每一步中，我们根据节点邻域中的一组值更新每个节点上的值。关键区别在于平均场消息传递方程在分布上而不是嵌入上运行，这在标准GNN消息传递中使用。

通过利用我们在第7.2.1节中介绍的希尔伯特空间嵌入，我们可以使GNN和平均场推断之间的联系更加紧密。假设我们有一些内射特征映射$\phi$，并且可以将所有边缘$q_v(\pmb{z}_v)$表示为嵌入
$$
\mu_v =\int_{\mathbb{R}^d} q_v(\pmb{z}_v) \phi(\pmb{z}_v)d\pmb{z}_v \in \mathbb{R}^d
$$
通过这些表示，我们可以将公式(86)中的不动点迭代改写为
$$
\mu_v^{(t)} = \pmb{c}+f(\mu_v^{(t-1)},\pmb{x}_v,\{\mu_u,\forall u \in \mathcal{N}(v)\})
$$
其中$f$是实值向量函数。注意，$f$聚集(aggregates)了来自相邻嵌入集的信息(例如:$\{\mu_u ,\forall u \in \mathcal{N}(v)\}$)并且使用这些聚合数据更新(updates)节点当前表示(例如:$\mu_v^{(t-1)}$)。通过这种方式，我们可以看到嵌入的平均场推理恰好对应于通过图形传递的神经信息的一种形式！

现在，在通常的概率建模场景中，我们将使用一些领域知识定义势函数$\Phi,\Psi$，以及特征映射$\phi$。给出一些$\Phi,\Psi,\phi$，然后我们可以试着解析地导出公式(89)中的$f$函数，这将允许我们使用嵌入版本的平均场推断。然而，作为替代方案，我们可以简单地尝试使用一些超监督信号以端到端的方式学习嵌入$\mu_v$，并且我们可以将$f$定义为任意神经网络。换句话说，我们不需要指定一个具体的概率模型，而可以简单地学习可能对应于某个概率模型的嵌入。$\mu_v$基于这一想法，Dai等人(P92)以类似于基本GNN的方式将$f$定义为
$$
\mu_v^{(t)} = \sigma \Big(\pmb{W}^{(t)}_{self} \pmb{x}_v+\pmb{W}_{neigh}^{(t)} \sum_{u \in \mathcal{N}(v)} \mu_u^{(t-1)}\Big)
$$
因此，在每次迭代中，节点$v$的更新Hilbert空间嵌入是其邻居嵌入及其特征输入的函数。并且，与基本GNN一样，更新过程的参数$\pmb{W}_{self}^{(t)}$和$\pmb{W}_{neigh}^{(t)}$可以通过梯度下降法在任意任务损失时进行训练。



### 7.2.4 GNNs and PGMs More Generally

在上一小节中，我们简要介绍了如何将基本GNN模型导出为平均场推断的嵌入形式(P92)。 然而，连接PGM和GNN还有更多的方法。例如，可以基于不同的近似推理算法导出消息传递的不同变体(P92)还有几项工作探讨了如何更普遍地将GNN集成到PGM模型中(P92)。总的来说，GNN和更传统的统计关系学习之间的联系是一个丰富的领域，具有巨大的新发展潜力。

## 7.3 GNNs and Graph Isomorphism

我们现在已经了解了如何基于图形信号处理和概率图形模型的连接来激发GNN。在本节中，我们将把注意力转向GNNs的第三个也是最后一个理论视角：基于图同构测试(graph isomorphism testing)连接的GNNs动机。

与前面的章节一样，这里我们将再次看到如何将基本GNN导出为现有算法的神经网络变体，在本例中为Weisfieler-Lehman（WL）同构算法。 However, in addition to motivating the GNN approach, connections to isomorphism testing will also provide us with tools to analyze the power of GNNs in a formal way.



### 7.3.1 Graph Isomorphism

图同构的检验是图论中最基本、研究最深入的任务之一。给定一对图$\mathcal{G}_1$和$\mathcal{G}_2$，图同构测试的目标是声明这两个图是否同构。从直觉上讲，两个图形同构意味着它们本质上是相同的。同构图表示完全相同的图结构，但它们可能仅在其相应邻接矩阵中节点的顺序上有所不同。形式上，如果我们有两个具有邻接矩阵$\pmb{A}_1$和$\pmb{A}_2$以及节点特征$\pmb{X}_2$和$\pmb{X}_1$的图，我们说这两个图是同构的当且仅当存在一个置换矩阵$\pmb{P}$，使得
$$
\pmb{PA}_1 \pmb{P}^T = \pmb{A}_2 \text{ and } \pmb{PX}_1 = \pmb{X}_2
$$
重要的是要注意，同构图在其底层结构方面是完全相同的。当我们使用代数对象（例如矩阵）表示图时，邻接矩阵中节点的顺序是我们必须做出的任意决定，但这种顺序与基础图本身的结构无关。

尽管图同构的定义很简单，但它的测试从根本上说是一个困难的问题。例如，测试同构的简单方法将涉及以下优化问题：
$$
\min_{\pmb{P} \in \mathcal{P}} \|\pmb{PA}_1\pmb{P}^T - \pmb{A}_2 \| +\| \pmb{PX}_1 - \pmb{X}_2\| \stackrel{?}{=} 0
$$
这种优化需要搜索整个置换矩阵集$\mathcal{P}$，以评估是否存在导致两个图之间等价的单个置换矩阵$\pmb{P}$。这种简单方法的计算复杂度在$O(|V|!)$是巨大的，事实上，没有多项式时间算法能够正确地测试一般图的同构。

图同构测试正式称为NP不确定（NPI）。众所周知，它不是NP完全问题，但没有通用的多项式时间算法。（整数因子分解是另一个被怀疑属于NPI类的众所周知的问题。）然而，有许多用于图同构测试的实用算法适用于广泛的图类，包括我们在第1章中简要介绍的WL算法。



### 7.3.2 Graph Isomorphism and Representational Capacity

图同构测试理论对于图表示学习特别有用。它为我们提供了一种量化不同学习方法代表性力量(representational power)的方法。例如，一个GNN-可以为图生成表示$\pmb{z}_{\mathcal{G}} \in \mathbb{R}^d$，那么我们可以通过询问这些表示对测试图同构有多有用来量化这种学习算法的能力。特别是，给定两个图的学习表示$\pmb{z}_{\mathcal{G}_1}$和$\pmb{z}_{\mathcal{G}_2}$，一个“完美”的学习算法将具有 
$$
\pmb{z}_{\mathcal{G}_1}= \pmb{z}_{\mathcal{G}_2} \text{ if and only if $\mathcal{G}_1$ is isomorphic to $\mathcal{G}_2$}
$$
一个完美的学习算法将为两个图生成相同的嵌入，当且仅当这两个图实际上是同构的。

当然，在实践中，没有一种表征学习算法是“完美的”（除非P=NP）。尽管如此，通过将表示学习算法与图同构测试相连接来量化表示学习算法的能力是非常有用的。尽管图同构测试通常是不可解的，但我们确实知道几种强大且易于理解的近似同构测试方法，通过将它们与这些方法进行比较，我们可以深入了解GNNs的威力。



### 7.3.3 The Weisfieler-Lehman Algorithm

将GNN连接到图同构测试的最自然的方法是基于与Weisfieler-Lehman（WL）算法家族的连接。在第一章中，我们讨论了graph kernel环境下的WL算法。然而，WL方法更广泛地被认为是近似同构测试中最成功和最广为人知的框架之一。通常称为1-WL的WL算法的最简单版本包括以下步骤：

1. 给定两个图$\mathcal{G}_1,\mathcal{G}_2$记最初的标签$l^{(0)}_{\mathcal{G}_i}(v)$对应每个图的每个节点。在大多数图中，这个标签就是节点的度，但是如果我们有关于节点的离散特征(例如：独热编码特征$\pmb{x}_v$)，那么我们能够用这些特征去定义最初的标签。

2. 接下来，我们通过散列节点邻域内的多组当前标签以及节点的当前标签，迭代地为每个图中的每个节点分配一个新标签：
   $$
   l_{\mathcal{G}_i}^{(t)}(v) = HASH(l_{\mathcal{G}_i}^{(t-1)}(v),\{\{l_{\mathcal{G}_i}^{(t-1)}(u) \ \ \forall u \in \mathcal{N}(v)\}\} )
   $$
   其中，双大括号用于表示multi-set，哈希函数将每个唯一的multi-set映射到一个唯一的新标签<font color=Red >**没懂这里的步骤**</font>

3. 我们重复步骤2，直到两个图中所有节点的标签收敛，也就是说，直到我们达到迭代K，其中$l_{\mathcal{G}_i}^{(K)}(v) =$ $l_{\mathcal{G}_i}^{(K-1)}(v),\forall v \in V_j ,j=1,2. $

4. 最后重构multi-sets
   $$
   L_{\mathcal{G}_j} = \{\{ l_{\mathcal{G}_i}^{(K)}(v), \forall v \in \mathcal{V}_j,i=0,...,K-1     \}\}
   $$
   总结每个图中的所有节点标签，我们声明$\mathcal{G}_1$和$\mathcal{G}_1$同构当且仅当两个图的多个集合相同，即当且仅当$L_{\mathcal{G}_1} = L_{\mathcal{G}_2} $。

图7.2显示了一张图上的WL标记过程示例。在每次迭代中，每个节点在其局部邻域中收集多组标签，并基于该多组标签更新自己的标签。在该标记过程的K次迭代之后，每个节点都有一个总结其K-hop邻域结构的标签，这些标签的集合可用于描述整个图或子图的结构。

<div align=center>
<img src="/Users/guass/Desktop/markdown/图神经网络/Graph Representation Learning/5png3.png" width="500" height=""/>
</div>









已知WL算法最多在$|\mathcal{V}|$迭代中收敛，并且已知它成功地测试了一大类图的同构[Babai和Kucera，1979]。然而，在一些众所周知的情况下，测试失败，如图7.3所示的简单示例。<font color=Red >**没懂为什么**</font>



<div align=center>
<img src="/Users/guass/Desktop/markdown/图神经网络/Graph Representation Learning/5png4.png" width="500" height=""/>
</div>

### 7.3.4 GNNs and the WL Algorithm

WL算法和神经消息传递GNN方法之间有明显的相似性。在这两种方法中，我们迭代地聚集来自本地节点邻域的信息，并使用这些聚集的信息来更新每个节点的表示。这两种方法之间的关键区别在于WL算法聚合和更新离散标签（使用哈希函数），而GNN模型使用神经网络聚合和更新节点嵌入。事实上，GNN是作为WL算法的一个连续的可微的模拟而被激发和导出的。

GNNs和WL算法（如第7.3.3节所述）之间的关系可以用以下定理形式化：

<font color=Aqua >**Theorem 4**</font> 将消息传递GNN（MP-GNN）定义为由以下形式的K个消息传递层组成的任何GNN：
$$
\pmb{h}_u^{(k+1)} = UPDATE^{(k)} \Big(\pmb{h}_u^{(k)}, AGGREGATE^{(k)}(\{\pmb{h}_v^{(k)},\forall v \in \mathcal{N}(u)\})\Big)
$$
其中AGGREGATE是可微置换不变函数，UPDATE是可微函数。此外，假设我们在初始层只有离散的特征输入，那么在K次迭代后，我们有$\pmb{h}_u^{(K)} \neq \pmb{h}_v^{(K)}$ 当且仅当$v,u$是不同的标签。



直观地说，定理4指出，当我们将离散信息作为节点特征时，GNN并不比WL算法更强大。如果WL算法将相同的标签分配给两个节点，那么传递GNN的任何消息也将向这两个节点分配相同的嵌入。这个关于节点标记的结果也扩展到同构测试。如果WL测试无法区分两个图，则MP-GNN也无法区分这两个图。我们还可以在另一个方向显示更积极的结果：

<font color=Aqua >**Theorem 5**</font> 存在一个MP-GNN有$\pmb{h}_u^{(K)} = \pmb{h}_v^{(K)}$当且仅当两个节点$v,u$在K次WL算法迭代后有相同的标签

该定理表明，存在与WL一样强大的消息传递GNN。

> **Which MP-GNNs are most powerful?** 
>
> 上述两个定理表明，消息传递GNN最多与WL算法一样强大，并且存在与WL算法一样强大的消息传递GNN。那么，哪个GNN实际获得了这个理论上界呢？有趣的是，我们在第5章开始介绍的基本GNN足以满足这个理论。特别是，如果我们将消息传递更新定义如下：
> $$
> \pmb{h}_u^{(k)} = \sigma \Big( \pmb{W}_{self}^{(k)} \pmb{h}_u^{(k-1)}+ \pmb{W}_{neigh}^{(k)} \sum_{v \in \mathcal{N}(u)} \pmb{h}_{v}^{(k-1)}+ \pmb{b}^{(k)}\Big)
> $$
> 那么这个GNN足以匹配WL算法的能力(P96)
>
> 然而，第5章讨论的大多数其他GNN模型没有WL算法强大。在形式上，为了与WL算法一样强大，聚合和更新函数需要是内射的(P96)。这意味着聚合和更新操作符需要将每个唯一的输入映射到唯一的输出值，这对于我们讨论的许多模型都不是这样。例如，使用相邻嵌入的（加权）平均值的聚合函数不是内射函数；如果所有的邻居都具有相同的嵌入，那么（加权）平均值将无法区分不同大小的输入集。
>
> (P96)详细讨论了各种GNN架构的相对能力。他们还定义了一个“最小”GNN模型，该模型的参数很少，但仍然与WL算法一样强大。他们将此模型称为图同构网络(Graph Isomorphism Network)（GIN），并通过以下更新定义：
> $$
> \pmb{h}_u^{(k)} = MLP^{(k)} \Big((1+\epsilon^{(k)}) \pmb{h}_u^{(k-1)}+ \sum_{v \in  \mathcal{N}(u)}\pmb{h}_v^{(k-1)}\Big)
> $$
> 其中$\epsilon^{(k)}$是可训练的参数



### 7.3.5 Beyond the WL Algorithm

上一小节强调了关于消息传递GNN（MP GNN）的一个重要负面结果：这些模型并不比WL算法更强大。然而，尽管有这一负面结果，研究如何使GNN比WL算法更强大是一个活跃的研究领域。

**Relational Pooling**

激励更强大的GNN的一种方法是考虑WL算法的失效情况。例如，我们可以在图7.3中看到，WL算法和所有MP GNN无法区分一个连接的六圈和一组两个三角形。从消息传递的角度来看，这种限制源于AGGREGATE和UPDATE操作无法检测两个节点何时共享一个邻居。在图7.3中的示例中，每个节点都可以从消息传递操作推断出它们有两个二级邻居，但该信息不足以检测节点的邻居是否相互连接。这一限制不仅仅是图7.3所示的corner情况。消息传递方法通常无法识别图中的闭合三角形，这是一个关键限制。

为了解决这个限制，(P97)考虑用唯一的节点ID特征增强MP-GNN。如果我们使用MP-GNN$(\pmb{A},\pmb{X})$表示输入邻接矩阵$\pmb{A}$和节点特征$\pmb{X}$上的任意MP-GNN，则添加节点ID相当于将MP-GNN修改为以下内容：
$$
MP-GNN(\pmb{A},\pmb{X} \oplus \pmb{I})
$$
其中$\pmb{I}$是$d \times d$维单位矩阵，且$\oplus$表示按列矩阵连接。换句话说，我们只需为每个节点添加一个唯一的、one-hot indicator。在图7.3的情况下，添加唯一的节点ID将允许MP-GNN识别两个节点何时共享一个邻居，这将使两个图区分开来。

然而，不幸的是，这种添加节点ID的想法并不能解决问题。事实上，通过添加唯一的节点ID，我们实际上引入了一个新的、同样有问题的问题：MP-GNN不再是置换等变的。对于标准MP-GNN，我们有
$$
\pmb{P}(MP-GNN(\pmb{A},\pmb{X})) = MP-GNN(\pmb{PAP}^T,\pmb{PX})
$$
其中$\pmb{P} \in \mathcal{P}$为任意置换矩阵。

这意味着标准MP GNN是置换等变的。如果我们排列邻接矩阵和节点特征，那么得到的节点嵌入只需以等价的方式排列即可。然而，具有节点ID的MP GNN通常不是置换不变的，因为
$$
\pmb{P}(MP-GNN(\pmb{A},\pmb{X} \oplus \pmb{I})) \neq  MP-GNN(\pmb{PAP}^T,\pmb{PX} \oplus \pmb{I})
$$
关键问题是，为每个节点分配一个唯一的ID会固定图的特定节点顺序，这会破坏置换等价性。

为了缓解这一问题，(P98)提出了关系池(Relational Pooling)（RP）方法，该方法涉及将所有可能的节点排列边缘化。给定任何MP-GNN，该GNN的RP扩展如下所示：
$$
PR-GNN(\pmb{A},\pmb{X}) = \sum_{\pmb{P} \in  \mathcal{P}} MP-GNN(\pmb{PAP}^T,(\pmb{PX}) \oplus \pmb{I})
$$
对所有可能的置换矩阵$\pmb{P} \in \mathcal{P}$求和恢复了置换不变性，并且我们保留了添加唯一节点ID的额外表示能力。事实上，(P98)证明了MP-GNN的RP扩展可以区分WL算法无法区分的图。

RP方法的局限性在于其计算复杂性。简单地计算上述公式的时间复杂度为$O(|\mathcal{V}|!)$，这在实践中是不可行的。然而，尽管存在这一限制，(P98)表明，RP方法可以通过使用各种近似值来降低计算成本（例如，对置换子集进行采样）来实现强大的结果。

**Thee k-WL test and k-GNNs**

上面讨论的关系池（RP）方法可以生成比WL算法更强大的GNN模型。然而，RP方法有两个关键限制：

- The full algorithm is computationally intractable.
-  We know that RP-GNNs are more powerful than the WL test, but we have no way to characterize *how much more* powerful they are.

为了解决这些限制，有几种方法考虑通过适应WL算法的推广来改进GNNs。

我们在第7.3.3节中介绍的WL算法实际上只是k-WL算法家族中最简单的一个。事实上，我们前面介绍的WL算法通常被称为1-WL算法，当$k>1$时，它可以推广到k-WL算法。k-WL算法背后的关键思想是，我们标记大小为k的子图，而不是单个节点。k-WL算法通过以下步骤生成图$\mathcal{G}$的表示：

1. 令$s = (u_1,u_2,...,u_k) \in \mathcal{V}^k$ 是定义大小为k的子图的元组，其中$u_1 \neq u_2 \neq... \neq u_k$。通过此子图的同构类定义每个子图的初始标签$l_{\mathcal{G}}^{(0)}(s)$（即，两个子图在且仅在同构时获得相同的标签）。

2. 接下来，我们通过散列(hashing)该子图邻域内的多组当前标签，迭代地为每个子图分配一个新标签：
   $$
   l_{\mathcal{G}}^{(i)}(s) = HASH(\{\{l_{\mathcal{G}}^{(i-1)}(s'),\forall s' \in \mathcal{N}_j(s),j = 1,...,k\}\},l_{\mathcal{G}}^{(i-1)}(s) )
   $$
   其中，第j个子图邻域定义为
   $$
   \mathcal{N}_j(s) = \{\{(u_1,...,u_{j-1},v,u_{j+1},...,u_k),\forall v \in \mathcal{V}\}\}
   $$
   双大括号用于表示multi-set

3. 我们重复步骤2，直到所有子图的标签收敛，也就是说，直到我们达到迭代K，其中每个k元组节点$s\in \mathcal{V}^k$均有$l_{\mathcal{G}}^{(K)}(s) =l_{\mathcal{G}}^{(K-1)}(s) $

4. 最后我们建立一个multi-set
   $$
   L_{\mathcal{G}} = \{\{l_{\mathcal{G}}^{(i)}(s) ,\forall s \in \mathcal{V}^k,i=0,...,K-1\}\}
   $$
   汇总图中的所有子图标签。<font color=Red >**这个子图到底是包含一些点的子图，还是不含不同类点的图**</font>



与1-WL算法一样，由k-WL算法生成的$L_{\mathcal{G}}$ muti-set可用于通过比较两个图的多集来测试图同构。还有基于k-WL测试的图核方法[Morris等人，2019]，类似于第1章中介绍的WL核。

关于k-WL算法的一个重要事实是，它引入了代表能力的层次结构。对于任何$k \geq 2$我们知道$(k+1)$-WL测试严格来说比k-WL测试更强大。因此，一个自然的问题是，我们是否可以设计出与$k>2$的k-WL测试同样强大的GNN，当然，一个自然的设计原则是通过类比k-WL算法来设计GNN。

k-GNN方法(P100)，它是k-WL算法的可微连续模拟。k-GNN学习与子图而非节点相关的嵌入，并且消息传递根据子图邻域发生（例如，如等式(104)中所定义）。k-GNNs可以像k-WL算法一样表达。然而，对于k-WL测试和k-GNNs，也存在严重的计算问题，因为消息传递的时间复杂性随着k的增加而组合爆炸。这些计算问题需要各种近似，以使k-GNN在实践中易于处理(P100)。



**Invariant and equivariant $k$-order GNNs**

另一个构建与k-WL test一样强大的GNN想法推动的工作是(P100)不变和等变 invariant and equivariant GNNs。消息传递GNN（MP-GNNs；如定理7.3.4中所定义）的一个关键方面是，它们与节点置换相同，这意味着
$$
\pmb{P}(MP-GNN(\pmb{A},\pmb{X})) = MP-GNN(\pmb{PAP}^T,\pmb{PX})
$$
对于任何置换矩阵$\pmb{P} \in \mathcal{P}$。这个等式表示，将输入置换到MP-GNN只会导致输出节点嵌入矩阵以类似的方式置换。

除了这个等变的概念外，我们还可以在图的层次上为MP-GNN定义一个类似的置换不变性的概念。特别是，MP-GNN可以通过$POOL: \mathbb{R}^{|\mathcal{V}| \times d} \rightarrow \mathbb{R}$函数进行扩展，它将学习节点嵌入$\pmb{Z} \in \mathbb{R}^{|\mathcal{V}| \times d}$的矩阵映射到整个图的嵌入$\pmb{z}_{\mathcal{G}} \in \mathbb{R}^d$。在这个图级设置中，我们知道MP-GNN是置换不变的，即
$$
POOL(MP-GNN(\pmb{PAP}^T,\pmb{PX})) = POOL(MP-GNN(\pmb{A},\pmb{X}))
$$
这意味着当使用不同的节点顺序时，池图级嵌入不会改变。

基于这种不变性和等变的思想，(P100)有一种基于置换等变/不变张量运算的一般形式的类GNN模型。假设我们有一个(k+1)阶张量$\mathcal{X} \in \mathbb{R}^{|\mathcal{V}|^k \times d}$，其中我们假设该张量的前$k$个通道/模式由图的节点索引。我们使用$\pmb{P} \star \mathcal{X}$来表示根据节点置换矩阵$\pmb{P}$置换该张量的前$k$个通道的操作。然后我们可以将线性等变层定义为线性算子(例如一个张量)$\mathcal{L}: \mathbb{R}^{|\mathcal{V}|^{k_1} \times d_1} \rightarrow \mathbb{R}^{|\mathcal{V}|^{k_2} \times d_2}$:
$$
\mathcal{L} \times (\pmb{P} \star \mathcal{X})  =\pmb{P} \star (\mathcal{L} \times \mathcal{X}), \forall \pmb{P} \in \mathcal{P}
$$
我们使用$\times$来表示一个广义的张量乘积。不变线性算子可以类似地定义为满足以下等式的张量$\mathcal{L}$：
$$
\mathcal{L} \times (\pmb{P} \star \mathcal{X})  =\mathcal{L} \times \mathcal{X}, \forall \pmb{P} \in \mathcal{P}
$$
请注意，等变线性算子和不变线性算子都可以表示为张量，但它们具有不同的结构。特别的，一个等变操作$\mathcal{L}: \mathbb{R}^{|\mathcal{V}|^{k} \times d_1} \rightarrow \mathbb{R}^{|\mathcal{V}|^{k} \times d_2}$对应于一个张量$\mathcal{L} \in \mathbb{R}^{|\mathcal{V}|^{2k} \times d_1 \times d_2}$，它有$2k$个被节点索引的通道。另一方面，一个不变操作$\mathcal{L}: \mathbb{R}^{|\mathcal{V}|^{k} \times d_1} \rightarrow \mathbb{R}^{ d_2}$对应于一个张量$\mathcal{L} \in \mathbb{R}^{|\mathcal{V}|^{k} \times d_1 \times d_2}$，它有$k$个被节点索引的通道。

有趣的是，从线性算子的张量观点来看，的等变（方程108）和不变（方程109）性质可以组合成一个要求，即$\mathcal{L}$张量是节点置换下的不动点：
$$
\pmb{P} \star \mathcal{L} = \mathcal{L}, \forall \pmb{P} \in \mathcal{P}
$$
换句话说，对于给定输入$\mathcal{X} \in \mathbb{R}^{|\mathcal{V}|^k \times d}$，该输入上的等变线性算子和不变线性算子都对应于满足方程(110)中不动点的张量，但张量中的通道数将根据它是等变算子还是不变算子而有所不同。

满足方程（110）中不动点的张量可以构造为一组固定基元素的线性组合。特别是，满足方程（110）的任何$l$阶张量$\mathcal{L}$可以写成
$$
\mathcal{L} = \beta_1 \mathcal{B}_1 + ....\beta_{b(l)} \mathcal{B}_{b(l)}
$$
 其中$\mathcal{B}_i$是一个固定的基本张量集合,$\beta_i \in \mathbb{R}$为实值权重,$b(l)$为第$l$个Bell number。

这些基张量的构造和推导涉及数学，与组合数学中的bell numbers理论密切相关。然而，一个关键的事实和挑战是，所需的基张量的数量随着第$l$个 bell numbers数的增加而增加，第$l$个bell numbers是一个指数增长序列。

利用这些线性等变层和不变层，基于以下函数组合定义了其不变k阶GNN模型
$$
MLP \circ \mathcal{L}_0\circ \sigma \circ \mathcal{L}_1 \circ  \sigma\mathcal{L}_2 ... \sigma \circ \mathcal{L}_m \times \mathcal{X}
$$
在这个组合中，我们应用了m个等变线性层$\mathcal{L}_1,...,\mathcal{L}_m$，其中$L_i : \mathcal{L}:\mathbb{R}^{|\mathcal{V}|^{k_i} \times d_1} \rightarrow \mathbb{R}^{|\mathcal{V}|^{k_{i+1}} \times d_2}$ 有$\max_i k_i = k $和$k_1 =2$。在这些线性等变层之间，一个元素非线性，记为$\sigma$。 构图中的倒数第二个函数是一个不变的线性层$\mathcal{L}_0$，然后是一个多层感知器（MLP）作为构图中的最终函数。k阶不变GNN的输入是张量$\mathcal{X}$，其中前两个通道对应于邻接矩阵，其余通道编码初始节点特征/标签。

这种方法被称为k阶，因为等变线性层包含多达k个不同通道的张量。然而，最重要的是，Maron等人[2019]证明了方程112之后的k阶模型与k-WL算法具有同等的威力。然而，与前一节讨论的k-GNN一样，为$k>3$构造k阶不变模型通常在计算上很难。



























