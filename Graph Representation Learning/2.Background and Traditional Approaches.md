<font color=LightCoral>**Problem:**</font>

<font color=LightBlue >**Solution:**</font>

<font color=Lime  >**Definition:**</font>

<font color=LightSalmon >**Remarks:**</font>

<font color=Aqua >**Lemma Theorem**</font>

<font color=Red >**Note:**</font>

<font color=DarkViolet >**Assumption:**</font>

<font color=HotPink >***Proof:***</font>

---

<center> <font size=6 ><b>2.Background and Traditional Apporaches</b></font></center> 

---



| discerning | 敏锐的 | differentiate |  区分  | paradigm | 范式 |
| :--------: | :----: | :-----------: | :----: | :------: | :--: |
| resultant  |  结果  |   implicit    | 含蓄的 |   ego    | 自我 |
|  pitfalls  |  陷阱  |  isomorphism  |  同构  |  motifs  | 主题 |
|  hashing   |  散列  | combinatorial | 组合的 |          |      |

## 2.1 Graph Statistics and Kernel Methods



这节主要在讲传统的机器学习方法处理图信息，构造表格特征，相当于时间序列做特征工程，放到普通的机器学习模型上（lgb、logistic等）。**图的特征工程**



### 2.1.1 Node-level statistics and features

- **Node degree**: $d_u$ for a node $u \in \mathcal{V}$  and simply counts the number of edges incident to a node:
  $$
  d_u = \sum_{v \in \mathcal{V}} \pmb{A}[u,v]
  $$
  
- **Node centrality**

  -  **eigenvector centrality**：eigenvector centrality also takes into account how important a node’s neighbors are.
    $$
    e_u= \frac{1}{\lambda} \sum_{v \in \mathcal{V}} \pmb{A}[u,v]e_v \quad \forall u \in \mathcal{V}
    $$
    上式等价于
    $$
    \lambda \pmb{e} = \pmb{Ae}
    $$
    One view of eigenvector centrality is that it ranks the likelihood that a node is visited on a random walk of infinite length on the graph. That is, since  $\lambda$ is the leading eigenvector of $\pmb{A}$, we can compute $\pmb{e}$ using power iteration via
    $$
    \pmb{e}^{(t+1)} = \pmb{Ae}^{(t)}
    $$
    If we start off this power iteration with the vector $\pmb{e}^{(0)} = (1,1,...,1)^T$, then we can see that after the first iteration $\pmb{e}^{(1)}$ will contain the degrees of all the nodes. At iteration $t,\pmb{e}^{(t)}$ will contain the number of length-$t$ paths arriving at each node. 

    Thus, by iterating this process indefinitely we obtain a score that is proportional to the number of times a node is visited on paths of infinite length.

  - **betweeness centrality**: which measures how often a node lies on the shortest path between two other nodes.

  - **closeness centrality**: which measures the average shortest path length between a node and all other nodes. 

  这些方法详见《Deep Learining of Graph》或者本书(P12)中的论文

- **The clustering coefficient**: which measures the proportion of closed triangles in a node’s local neighborhood. The popular *local variant* of the clustering coefficient is computed as follows：
  $$
  c_u  = \frac{|(v_1,v_2) \in \mathcal{E}: v_1,v_2 \in \mathcal{N}(u)|}{\begin{pmatrix} d_u \\ 2\end{pmatrix}}
  $$
  The numerator in this equation counts the number of edges between neighbours of node $u$. ($\mathcal{N}(u)= $ $\{v \in \mathcal{V}:(u,v) \in \mathcal{E} \}$ to  denote the node neighborhood). The denominator calculates how many pairs of nodes there are in $u$'s neighborhood. $d_u$是$u$的度：邻居的个数 

- **Closed triangles, ego graphs, and motifs.** 从上面的方法可以从triangle扩展到cycle即不同周期的长度。由此we can essentially transform the task of computing node-level statistics and features to a graph-level task.

### 2.1.2 Graph-level features and graph kernels

define implicit kernels的相关论文(P14)

- **Bag of nodes** The simplest approach to defining a graph-level feature is to just aggregate node level statistics. （点的degrees, centralities的聚合信息），缺点是只用了点的信息，可能会忽视图的全局性质。

- **The Weisfeiler-Lehman kernel**: One way to improve the basic bag of nodes approach is using a strategy of **iterative neighborhood aggregation.** WL是iterative neighborhood aggregation方法的一种. The basic idea behind the **WL** algorithm is the following: 

  1.  we assign an initial label $l^{(0)}(v)$ to each node. In most graphs, this label is simply the degree, i.e., $l^{(0)}(v) = d_v \quad v \in \mathcal{V}$

  2.  Next, we iteratively assign a new label to each node by hashing the multi-set of the current labels within the node’s neighborhood:
     $$
     l^{(i)}(v) = HASH(\{\{l^{(i-1)}(u) \quad u \in \mathcal{N}(v)\}\})
     $$
     where the double-braces are used to denote a multi-set and the HASH function maps each unique multi-set to a unique new label. <font color=Red >**不懂**</font>

  3. After running $K$ iterations of re-labeling (i.e., Step 2), we now have a label $l^{(K)}(v)$  for each node that summarizes the structure of its $K$-hop neighborhood. We can then compute histograms or other summary statistics over these labels as a feature representation for the graph. In other words, the WL kernel is computed by measuring the **difference between the resultant label sets for two graphs**.

  该方法可以解决the isomorphism problem for a broad set of graphs (P15)

- **Graphlets and path-based methods**

  Formally, the graphlet kernel involves enumerating all possible graph structures of a particular size and counting how many times they occur in the full graph.

  Figure 2.2 illustrates the various graphlets of size 3, The challenge with this approach is that counting these graphlets is a combinatorially difficult problem.

<div align=center>
<img src="/Users/guass/Desktop/markdown/图神经网络/Graph Representation Learning/2png1.png" width="500" height=""/>
</div>
  An alternative to enumerating all possible graphlets is to use **path-based methods**.
   (random walk kernel,shortest-path kernel ) 相关论文(P15)

This idea of char acterizing graphs based on walks and paths is a powerful one, as it can extract

**rich structural information** while avoiding many of the **combinatorial pitfalls** of graph data.



## 2.2 Neighborhood Overlap Detection

上节所讲的特征提取，对于分类任务很有用，However, they are limited in that they do not quantify the **relationships** between nodes.

<div align=center>
<img src="/Users/guass/Desktop/markdown/图神经网络/Graph Representation Learning/2png2.png" width="600" height=""/>
</div>

In this section we will consider **various statistical measures of neighborhood overlap between pairs of nodes**, which quantify the extent to which a pair of nodes are related.

The simplest neighborhood overlap measure just counts the number of neighbors that two nodes share:
$$
\pmb{S}[u,v] = |\mathcal{N}(u) \cap \mathcal{N}(v)| 
$$
where we use $\pmb{S}[u,v]$ to denote the value quantifying the relationship between nodes $u$ and $v$ and let $\pmb{S} \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|}$ denote the **similarity matrix** summarizing all the pairwise node statistics.

Given a neighborhood overlap statistic $\pmb{S}[u,v]$ a common strategy is to assume that the likelihood of an edge $(u,v)$ is simply proportional to $\pmb{S}[u,v]$:
$$
P(\pmb{A}[u,v] = 1) \propto \pmb{S}[u,v]
$$

### 2.2.1 Local overlap measures

Local overlap statistics are simply functions of the number of common neighbors two nodes share, i.e. $|\mathcal{N}(u) \cap \mathcal{N}(v)|$.

- **Sorensen index** $\pmb{S}_{Sorenson} \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|}$ of node-node neighborhood overlaps with entries given by
  $$
  \pmb{S}_{Sorenson} [u,v] = \frac{2|\mathcal{N}(u) \cap \mathcal{N}(v)|}{d_u +d_v}
  $$

- **Salton index**
  $$
  \pmb{S}_{Salton}[u,v]  =\frac{2|\mathcal{N}(u) \cap \mathcal{N}(v)|}{\sqrt{d_u d_v}}
  $$

- **Jaccard overlap**
  $$
  \pmb{S}_{Jaccard}[u,v] = \frac{|\mathcal{N}(u) \cap \mathcal{N}(v)|}{|\mathcal{N}(u) \cup \mathcal{N}(v)|}
  $$

There are also measures that go beyond simply counting the number of common neighbors and that seek to consider the **importance** of common neighbors in some way.

- **Resource Allocation(RA) index** counts the inverse degrees of the common neighbors
  $$
  \pmb{S}_{RA}[v_1,v_2] = \sum_{u \in \mathcal{N}(v_1) \cap \mathcal{N}(v_2)}\frac{1}{d_u}
  $$

- **Adamic-Adar(AA) index** performs a similar computation using the inverse logarithm of the degrees:
  $$
  \pmb{S}_{AA}[v_1,v_2]  = \sum_{u \in \mathcal{N}(v_1) \cap \mathcal{N}(v_2)}\frac{1}{\log(d_u)}
  $$

Both these measures give more weight to common neighbors that have low degree, with intuition that a shared low-degree neighbor is more informative than a shared high-degree one.



### 2.2.2 Global Overlap Measures

The local approaches are limited in that they only consider local node neighborhoods.

For example, two nodes could have no local overlap in their neighborhoods but still be members of the same community in the graph. **Global overlap** statistics attempt to take such relationships into account.

- **Katz index**  To compute the Katz index we simply count the number of paths **of all lengths** between a pair of nodes:
  $$
  \pmb{S}_{Katz}[u,v]  =\sum_{i=1}^\infty \beta^i \pmb{A}^i [u,v]
  $$
  where $\beta \in \mathbb{R}^+$is a user-defined parameter controlling how much weight is given to short versus long paths.

  ><font color=Aqua >**Theorem 1**</font> let $\pmb{X}$  be a real-valued square matrix and let $\lambda_1$ denote the largest eigenvalue of $\pmb{X}$. Then 
  >$$
  >(\pmb{I-X})^{-1} = \sum_{i=0}^\infty \pmb{X}^i
  >$$
  >
  >if and only if $\lambda_1 < 1$ and $(\pmb{I-X})$ is non-singular.
  
  Based on Theorem 1, we can see that the solution to the Katz index is given by
  $$
  \pmb{S}_{Katz} = (\pmb{I} - \beta \pmb{A})^{-1} -\pmb{I}
  $$
  
- **Leicht, Holme, and Newman (LHN) similarity** 

  One issue with the Katz index is that it is strongly biased by node degree.

  Considering the ratio between the actual number of observed paths and the **number of expected paths between two nodes**:
  $$
  \frac{\pmb{A}^i}{\mathbb{E}[\pmb{A}^i]}
  $$
  To compute the expectation $\mathbb{E}[\pmb{A}^i]$, we rely on what is called the **configuration model**, which assumes that we draw a random graph with the same set of degrees as our given graph. 由此有
  $$
  \mathbb{E}[\pmb{A}[u,v]]  = \frac{d_u d_v}{2m}
  $$
  $m = |\mathcal{E}|$  to denote the total number of edges in the graph.
  $$
  \mathbb{E}[\pmb{A}^2[v_1,v_2]] = \frac{d_{v_1}d_{v_2}}{(2m)^2} \sum_{v \in \mathcal{V}} (d_u -1)d_u
  $$
  对于公式的解释见(P20), 随着路径长度超过3，在随机配置模型下，对预期节点路径的分析计算变得难以处理。最大特征值可以用来近似路径数量的增长。If we define $\pmb{p}_i \in \mathbb{R}^{|\mathcal{V}|}$ as the vector counting the number of length-$i$ paths between node $u$ and all other nodes, then we have that for large $i$
  $$
  \pmb{A p}_i = \lambda_1 \pmb{p}_{i-1}
  $$
  since $\pmb{p}_i$ will eventually converge to the dominant eigenvector of the graph. This  implies that the number of paths between two nodes grows by a factor of $\lambda_1$ at each iteration. Based on this approximation for large $i$ as well as the exact solution for $i = 1$ we obtain:
  $$
  \mathbb{E} [\pmb{A}^i [u,v]] = \frac{d_u d_v \lambda^{i-1}}{2m}
  $$
  <font color=Red >**Note:**</font> 这里$\lambda$应该是有下标的$\lambda_1$

  最后我们获得了normalized version of the Katz index, which we term the **LNH index**
  $$
  \pmb{S}_{LNH}[u,v] = \pmb{I}[u,v] +\frac{2m}{d_v d_u} \sum_{i=0}^\infty \beta^i \lambda_1^{1-i} \pmb{A}^i [u,v]
  $$
  根据定理1,得到
  $$
  \pmb{S}_{LNH} = 2 \alpha m \lambda_1 \pmb{D}^{-1}(\pmb{I} - \frac{\beta}{\lambda_1}\pmb{A})^{-1}\pmb{D}^{-1}
  $$
  $\pmb{D}$ is a matrix with node degrees on the diagonal.

- **Random walk methods** 一种PageRank算法的变体：Personalized PageRank algorithm(P21). 定义$\pmb{P} = \pmb{AD}^{-1}$ and compute:
  $$
  \pmb{q}_u = c  \pmb{Pq}_u  + (1-c) \pmb{e}_u
  $$
  
  $\pmb{e}_u$ is a one-hot indicator vector for node $u$ and $\pmb{q}_u [v]$ gives the stationary probability that random walk starting at node $u$ visits node $v$. 
  
  Here, the $c$ term determines the probability that the random walk restarts at node $u$ at each timestep. The solution to this recurrence is given by
  $$
  \pmb{q}_u = (1-c) (\pmb{I} - c\pmb{P})^{-1}\pmb{e}_u
  $$
  and we can define a node-node random walk similarity measure as
  $$
  \pmb{S}_{RW}[u,v] = \pmb{q}_u [v] +\pmb{q}_v [u]
  $$
  

## 2.3 Graph Laplacian and Spectral Methods

2.1节讲的是图分类的传统方法，2.2讲的是关系预测的传统方法，2.3讲图的节点聚类

### 2.3.1 Graph Laplacians

Laplacians are formed by various transformations of the adjacency matrix.

- **Unnormalized Laplacian** The most basic Laplacian matrix is the unnormalized Laplacian, defifined as

  follows: 
  $$
  \pmb{L= D-A}
  $$
  $\pmb{A}$是邻接矩阵，$\pmb{D}$是对角度矩阵， The Laplacian matrix of a simple graph has a number of important properties:

  1. It is symmetric and positive semi-definite

  2. The following vector identity holds $\forall \pmb{x} \in \mathbb{R}^{|\mathcal{V}|}$
     $$
     \pmb{x}^T \pmb{Lx} = \frac{1}{2} \sum_{u \in \mathcal{V}} \sum_{v \in \mathcal{V}} \pmb{A}[u,v] (\pmb{x}[u]-\pmb{x}[v])^2 = \sum_{(u,v) \in \mathcal{E}} (\pmb{x}[u] - \pmb{x}[v])^2
     $$

  3. $\pmb{L}$  has $|\mathcal{V}|$ non-negative  eigenvalues: $ 0 = \lambda_{|\mathcal{V}|} \leq \lambda_{|\mathcal{V}|-1}\leq ...\leq \lambda_1$

  ><font color=Aqua >**Theorem 2**</font> The geometric multiplicity of the 0 eigenvalue of the Laplacian $\pmb{L}$ corresponds to the number of connected components in the graph.

  <font color=Red >**Note:**</font> 关于这部分可以参考另一本书第二章笔记。或者(P23)

- **Normalized Laplacians** 两种分别是: symmetric normalized Laplacian
  $$
  \pmb{L}_{sym}=  \pmb{D}^{-\frac{1}{2} } \pmb{LD}^{-\frac{1}{2} }
  $$
  random walk Laplacian
  $$
  \pmb{L}_{RW} = \pmb{D}^{-1}\pmb{L}
  $$
  

### 2.3.2 Graph Cuts and Clustering

定理2说明可以使用Laplacian矩阵对点进行分类. However, this approach only allows us to cluster nodes that are already in disconnected components, which is trivial. 本节将展示Laplacian can be used to give an optimal clustering of nodes **within a fully connected graph.**

- **Graph Cuts** 首先要定义什么是**optimal cluster**，首先定义**cut**

  >  <font color=Lime  >**Cut on a Graph :**</font> $\mathcal{A} \subset \mathcal{V}$ denote a subset of the nodes in the graph and let $\bar{\mathcal{A}}$ denote the complement of this set, i.e., $\mathcal{A} \cup \bar{\mathcal{A}} = \mathcal{V}, \mathcal{A} \cap \bar{\mathcal{A}} = \empty$. Given a partitioning of the graph into $K$ non-overlapping subsets $\mathcal{A}_1,...,\mathcal{A}_K$ we define the cut value of this partition as 

  $$
  cut(\mathcal{A}_1,..,\mathcal{A}_K) = \frac{1}{2} \sum_{k=1}^K |(u,v) \in \mathcal{E}: u \in \mathcal{A}_k, v \in \bar{\mathcal{A}_k}|
  $$

  因此一种定义optimal cluster的方法是最小化cut value，有很多算法解决这个任务，但问题是容易产生单个节点组成的集群。

  改进**Ratio Cut :**
  $$
  RatioCut(\mathcal{A}_1,...,\mathcal{A}_K) = \frac{1}{2} \sum_{k=1}^K \frac{|(u,v) \in \mathcal{E}: u \in \mathcal{A}_k,v \in \bar{\mathcal{A}_k|}}{|\mathcal{A}_k|}
  $$
  **Normalized Cut (Ncut)**:
  $$
  NCut(\mathcal{A}_1,...,\mathcal{A}_K) = \frac{1}{2} \sum_{k=1}^K \frac{|(u,v) \in \mathcal{E}: u \in \mathcal{A}_k,v \in \bar{\mathcal{A}_k|}}{vol(\mathcal{A}_k)}
  $$
  where $vol(\mathcal{A} ) = \sum_{u \in \mathcal{A}} d_u$

- **Approximating the RatioCut with the Laplacian spectrum** 使用Laplacian spectrum最小化 RatioCut也可以用于最小化Ncut

  考虑$K=2$的情况，目标是最小化函数
  $$
  \min_{\mathcal{A} \in \mathcal{V}} RatioCut(\mathcal{A},\bar{\mathcal{A}})
  $$
  定义 $\pmb{a} \in \mathbb{R}^{|\mathcal{V}|}$:
  $$
  \pmb{a}[u] = \begin{cases} \sqrt{\frac{|\bar{\mathcal{A}}|}{|\mathcal{A}|}} \quad \text{if } u \in \mathcal{A} \\
  -\sqrt{ \frac{|\mathcal{A}|}{|\bar{\mathcal{A}}|}} \quad \text{of } u \in \bar{\mathcal{A}}\end{cases}
  $$
  Then we have
  $$
  \pmb{a}^T \pmb{La} = \sum_{(u,v) \in \mathcal{E}} (\pmb{a}[u] - \pmb{a}[v])^2 \\
  =  \sum_{(u,v) \in \mathcal{E}: u \in \mathcal{A},v \in \bar{\mathcal{A}}} (\pmb{a}[u] - \pmb{a}[v])^2 \\  \sum_{(u,v) \in \mathcal{E}: u \in \mathcal{A},v \in \bar{\mathcal{A}}} (\sqrt{\frac{|\bar{\mathcal{A}}|}{|\mathcal{A}|}}  -( -\sqrt{\frac{|\mathcal{A}|}{|\bar{\mathcal{A}}|}}))^2 \\
  = cut(\mathcal{A},\bar{\mathcal{A}}) (\frac{|\mathcal{A}|}{|\bar{\mathcal{A}}|} +\frac{|\bar{\mathcal{A}}|}{|\mathcal{A}|}+2) \\
  = cut(\mathcal{A},\bar{\mathcal{A}}) (\frac{|\mathcal{A}+\bar{\mathcal{A}}|}{|\bar{\mathcal{A}}|} +\frac{|\bar{\mathcal{A}}+\mathcal{A}|}{|\mathcal{A}|}+2) \\
  = |\mathcal{V}| RatioCut(\mathcal{A},\bar{\mathcal{A}})
  $$
  $\pmb{a}$有两个重要的性质，$\mathbb{1}$ is the vector of all ones.
  $$
  \sum_{v \in \mathcal{V}} \pmb{a}[u] = 0 \Leftrightarrow \pmb{a} \perp \mathbb{1} \\
  \| \pmb{a} \| ^2 = |\mathcal{V}|
  $$
  由此我们得到上述最优化问题
  $$
  \min_{\mathcal{A} \subset \mathcal{V}} \pmb{a}^T \pmb{La} \qquad s.t. \ \pmb{a} \perp \mathbb{1} \  \& \
  \| \pmb{a} \| ^2 = |\mathcal{V}|
  $$
  
  根据(35)$\pmb{a}$的定义这是个NP-hard问题，因为以上公式是在离散集合上求解，要转为实值向量
  $$
  \min_{\pmb{a} \in \mathbb{R}^{|\mathcal{V}|}} \pmb{a}^T \pmb{La} \qquad s.t. \ \pmb{a} \perp \mathbb{1} \  \& \
  \| \pmb{a} \| ^2 = |\mathcal{V}|
  $$
  根据 Rayleigh-Ritz Theorem，该问题的解可以由 second-smallest eigenvector of $\pmb{L}$得到（对应的是第二小特征值）.
  
  Thus, we can approximate the minimization of the RatioCut by setting $\pmb{a}$ to be the second-smallest eigenvector of the Laplacian. 为了将实向量转为离散任务，可以做如下操作
  $$
  \begin{cases}u \in \mathcal{A} \quad \text{if } \pmb{a}[u] \geq 0 \\
  u \in \bar{\mathcal{A}} \quad \text{if } \pmb{a}[u] < 0  \end{cases}
  $$
  In summary, the second-smallest eigenvector of the Laplacian is a continuous approximation to the discrete vector that gives an optimal cluster assignment, 类似的结果在NCut值，详见(P26)



### 2.3.3 Generalized Spectral Clustering

上节看到 the spectrum of the Laplacian allowed us to find a meaningful partition of the graph into two clusters. 该想法可以扩展到任意的$K$个聚类通过使用$K$个拉普拉斯阵中最小的特征向量。该问题的步骤为：



1. Find the $K$ smallest eigenvectors of $\pmb{L}$  (excluding the smallest): $\pmb{e}_{|\mathcal{V}|-1},\pmb{e}_{|\mathcal{V}|-2},...,\pmb{e}_{|\mathcal{V}|-K}$.

2. Form the matrix $\pmb{U} \in \mathbb{R}^{|\mathcal{V}| \times (K-1)}$ with the eigenvectors from Step 1 as columns.

3. Represent each node by its corresponding row in the matrix $\pmb{U}$, i.e.,
   $$
   \pmb{z}_u = \pmb{U}[u] \quad \forall u \in \mathcal{V}
   $$

4. Run $K$-means clustering on the embeddings $\pmb{z}_u  \forall u \in \mathcal{V}$



## 2.4 总结

本章中讨论的方法，特别是节点和图级统计，由于需要仔细、手工设计的统计和度量，因此受到限制。这些手工设计的功能是不灵活的，也就是说，它们无法通过学习过程进行调整，而设计这些功能可能是一个耗时且昂贵的过程。本书的以下章节介绍了通过图形学习的替代方法：*图形表示学习。*我们将寻求*学习*编码图形结构信息的表示，而不是提取手工设计的特征。





























