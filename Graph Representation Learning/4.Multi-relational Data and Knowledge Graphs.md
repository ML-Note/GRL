<font color=LightCoral>**Problem:**</font>

<font color=LightBlue >**Solution:**</font>

<font color=Lime  >**Definition:**</font>

<font color=LightSalmon >**Remarks:**</font>

<font color=Aqua >**Lemma Theorem**</font>

<font color=Red >**Note:**</font>

<font color=DarkViolet >**Assumption:**</font>

<font color=HotPink >***Proof:***</font>

---

<center> <font size=6 ><b>4.Multi-relational Data and Knowledge Graphs</b></font></center> 

---



| complication | 并发症 | adversarial | 对抗的 | augments  | 增加 |
| :----------: | :----: | :---------: | :----: | :-------: | :--: |
|  exhaustive  | 详尽的 | augmenting  |  增强  | filtering | 过滤 |
|              |        |             |        | entities  | 实体 |
|              |        |             |        |           |      |



**Knowledge graph completion**:  we are given a multi-relational graph $\mathcal{G} = (\mathcal{V},\mathcal{E})$, where the edges are defined as tuples $e = (u,\tau,v)$  indicating the presence of a particular relation $\tau \in \mathcal{T}$ holding between two nodes.

这样的 multi-relational graphs are often referred to as **knowledge graphs**， 因为我们可以将元组$(u,\tau,v)$解释为指定两个节点$u$和$v$之间存在一个特定的“事实”。

Generally the goal in knowledge graph completion is to predict missing edges in the graph, i.e., relation prediction, but there are also examples of node classification tasks using multi-relational graphs(P38)

这章将介绍embedding methods for multi-relational graphs. 并不介绍完整的知识图谱，也并不介绍所有的embedding变种，了解这部分内容可以参考论文(P38)



## 4.1 Reconstructing multi-relational data

与之前不同的是，现在要处理多种类型的边。为了解决这个问题，增强decoder使得可以处理multi-relational 问题。

Instead of only taking a pair of node embeddings as input, we now define the decoder as accepting a pair of node embeddings **as well as a relation type**. i.e $DEC:\mathbb{R}^d \times \mathcal{R} \times \mathbb{R}^d \rightarrow \mathbb{R}^+$,  one of the simplest and earliest approaches to learning multi-relational embeddings—often termed **RESCAL**—defined the decoder as (P39)
$$
DEC(u,\tau,v) = \pmb{z}_u^T \pmb{R}_\tau \pmb{z}_v
$$
where $\pmb{R}_\tau \in \mathbb{R}^{d \times d}$ is a learnable matrix specific to relation $\tau \in \mathcal{R}$ 

we could train our embedding matrix $\pmb{Z}$ and our relation matrices $\pmb{R}_\tau, \forall \tau \in \mathcal{R}$ using a basic reconstruction loss:
$$
\mathcal{L} = \sum_{u \in \mathcal{V}} \sum_{v \in \mathcal{V}} \sum_{\tau \in \mathcal{R}} \| DEC(u,\tau,v) - \mathcal{A}[u,\tau,v] \| ^2  = \sum_{u \in \mathcal{V}} \sum_{v \in \mathcal{V}} \sum_{\tau \in \mathcal{R}} \| \pmb{z}_u^T \pmb{R}_\tau \pmb{z}_v - \mathcal{A}[u,\tau,v] \| ^2
$$
where $\mathcal{A} \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{R}| \times |\mathcal{V}|}$ is the adjacency tensor for the multi-relational graph.如果我们要优化方程（2），我们实际上会执行一种**张量分解 tensor factorization**。因此，张量分解的思想推广了第3章中讨论的矩阵分解方法。

**Loss functions, decoders, and similarity functions**:

In Chapter 3 we discussed how the diversity of methods for node embeddings largely stem from the use of different **decoders ($DEC$)** which gives a score between a pair of node embeddings;, **similarity measures ($\pmb{S}[u,v]$)** which is the similarity function defines what kind of node-node similarity we are trying to decode, and **loss functions($\mathcal{L}$)** which tells us how to evaluate the discrepancy between the output of the decoder and the ground truth similarity measure.

Nearly all multi-relational embedding methods simply define the **similarity measure** directly based on the adjacency tensor.This is due to the difficulty of defining higher-order neighborhood relationships in multi-relational graphs 同样也是因为most multi-relational embedding methods were specifically designed for relation prediction.



## 4.2 Loss Functions

multi-relational node embedding method主要的两个方面就是decoder和loss function

公式(2)中所讨论的simple reconstruction loss有两个主要缺点。1. 计算消耗巨大，时间复杂度$O(|\mathcal{V}|^2|\mathcal{R}|)$, 另外由于很多 multi-relational graphs是稀疏的，也就是$|\mathcal{E}| \ll |\mathcal{V}|^2|\mathcal{R}|$—we would ideally want a loss function that is $O(|\mathcal{E}|)$  2.  我们的目标是从低维节点嵌入中解码邻接张量。我们知道（在大多数情况下）这个张量只包含二元值，但等式（2）中的均方误差并不适合这种二元比较。（这是个小问题）

- **Cross-entropy with negative sampling：**One popular loss function that is both efficient and suited to our task. We define this loss as:
  $$
  \mathcal{L} = \sum_{(u,\tau,v) \in \mathcal{E}} -\log (\sigma(DEC(\pmb{z}_u,\tau,\pmb{z}_v)))- \gamma \mathbb{E}_{v_n \sim P_{n,u}(\mathcal{V})} [\log (\sigma(-DEC(\pmb{z}_u,\tau,\pmb{z}_{v_n})))]
  $$
  where $\sigma$ denotes the logistic function, $P_{n,u}(\mathcal{V})$ denotes a "negative sampling" distribution over the set of nodes $\mathcal{V}$ (which might depend on $u$) and $\gamma > 0 $ is a hyperparameter. 这与第3章公式(13)是类似的，只不过扩展到了multi-relational decoders.

   we obtain normalized scores in [0*,* 1] that can be interpreted as probabilities. The term
  $$
  \log (\sigma(DEC(\pmb{z}_u,\tau,\pmb{z}_v)))
  $$
  then equals the log-likelihood that we predict “true” for an edge that does actually exist in the graph.  On the other hand, the term 
  $$
   \mathbb{E}_{v_n \sim P_{n,u}(\mathcal{V})} [\log (\sigma(-DEC(\pmb{z}_u,\tau,\pmb{z}_{v_n})))]
  $$
  then equals the expected log-likelihood that we correctly predict “false” for an edge that does not exist in the graph. 

  In practice, the expectation is evaluated using a Monte Carlo approximation and the most popular form of this loss is
  $$
  \mathcal{L} = \sum_{(u,\tau,v) \in \mathcal{E}} \Big( -\log ( \sigma(DEC(\pmb{z}_u,\tau,\pmb{z}_v)) ) - \sum_{v_n \in \mathcal{P}_{n,v}} [\log (\sigma(-DEC(\pmb{z}_u,\tau,\pmb{z}_{v_n})))] \Big)
  $$
  where $\mathcal{P}_{n,u}$ is a  (usually small) set of nodes sampled from $P_{n,v}(\mathcal{V})$

  > **A note on negative sampling**: 如何进行负采样会很大的影响embedding的学习情况，常用的方法是定义分布$P_{n,u}$is to simply use a uniform distribution over all nodes in the graph. 这个方法会导致我们会得到 “false negatives” in the cross-entropy calculation. 也就是采样到了真的样本。可以过滤掉真样本。
  >
  > 其他负采样变种尝试产生更"difficult"的负样本. 比如一些联系只能存在于特定种类的节点，因此一种策略是负样本仅满足这样的限制。Sun等人[2019]甚至提出了一种通过学习对抗模型来选择具有挑战性的负面样本的方法。(P41)
  >
  > Note also that—without loss of generality—we have assumed that the negative sampling occurs over the second node in the edge tuple. That is, we assume that we draw a negative sample by replacing the **tail** node $v$  in the tuple $(u,\tau,v)$ with a negative sample $v_n$.  Always sampling the tail node simplifies notation but can lead to biases in multi-relational graphs where edge direction is important. In practice it can be better to draw negative samples for both the **head** node i.e., $u$ and the **tail** node i.e. $v$ of the relation.



- **Max-margin loss** : The other popular loss function used for multi-relational node embedding is the

  margin loss: 
  $$
  \mathcal{L} = \sum_{(u,\tau,v) \in \mathcal{E}} \sum_{v_n \in \mathcal{P}_{n,v}} \max (0, -DEC(\pmb{z}_u,\tau,\pmb{z}_v)+DEC(\pmb{z}_u,\tau,\pmb{z}_{v_n})+ \Delta)
  $$
  In this loss we are again comparing the decoded score for a true pair compared to a negative sample—a strategy often termed **contrastive estimation.** 公式(7)对比的是the direct output of the decoders而不是将其以二元分类任务对待.  This loss is also known as the **hinge loss**.



## 4.3  Multi-relational decoders

we have only discussed one possible multi-relational decoder, the so-called **RESCAL decoder**, which was introduced in Section 4.1:
$$
DEC(\pmb{z}_u ,\tau,\pmb{z}_v) = \pmb{z}_u^T \pmb{R}_\tau \pmb{z}_v
$$
 we associate a trainable matrix $\pmb{R}_\tau \in \mathbb{R}^{d \times d}$ with each relation. 这个算法 high computational and statistical cost for representing relations. There are $O(d^2)$ parameters for each relation type in RESCAL, which means that relations require an order of magnitude more parameters to represent, compared to entities.

接下来将介绍几个流行的decoder方法只需训练$O(d)$个参数来表示每一个关系。

<div align=center>
<img src="/Users/guass/Desktop/markdown/图神经网络/Graph Representation Learning/4png1.png" width="600" height=""/>
</div>

- **Translational decoders(TransE)** (P42)
  $$
  DEC(\pmb{z}_u,\tau,\pmb{z}_v) = - \|\pmb{z}_u + \pmb{r}_\tau - \pmb{z}_v \|
  $$
  在这些方法中，我们使用d维嵌入来表示每个关系。The likelihood of an edge is proportional to the distance between the embedding of the head node and the tail node, after translating the head node according to the relation embedding.  TransE是最早的 multi-relational decoders之一，并且如今也是在许多应用中的baseline。

  TransE的一个缺陷是它太简单了，一个改进**TransX**
  $$
  DEC(\pmb{z}_u,\tau,\pmb{z}_v) = - \|g_{1,\tau}(\pmb{z}_u) + \pmb{r}_\tau - g_{2,\tau}(\pmb{z}_v) \|
  $$
  where $g_{i,\tau}$ are trainable transformations that depend on the relation $\tau$. 例如**TransH** model(P43)
  $$
  DEC(\pmb{z}_u,\tau,\pmb{z}_v) = - \|(\pmb{z}_u - \pmb{w}_r^T \pmb{z}_u \pmb{w}_r) + \pmb{r}_\tau - (\pmb{z}_v - \pmb{w}_r^T \pmb{z}_v \pmb{w}_r) \|
  $$
  TransH方法在执行转换之前，将实体嵌入到法向量wr定义的可学习关系特定超平面上。<font color=Red >**不懂**</font> 变种见(P43)

- **Multi-linear dot products** Rather than defifining a decoder based upon translating embeddings, a second popular line of work develops multi-relational decoders by generalizing the dot product decoder from simple graphs.
  $$
  DEC(\pmb{z}_u,\tau,\pmb{z}_v) = <\pmb{z}_u,\pmb{r}_\tau,\pmb{z}_v> = \sum_{i=1}^d \pmb{z}_u[i] \times \pmb{r}_\tau [i] \times \pmb{z}_v [i]
  $$
  
  Thus, this approach takes a straightforward generalization of the dot product to be defined over three vectors. 该方法也叫做**DistMult decoder**
  
- **Complex decoders** 上述方法的一个缺点是只能encode symmertric relations.也就是有
  $$
  DEC(\pmb{z}_u,\tau,\pmb{z}_v) = DEC(\pmb{z}_v,\tau,\pmb{z}_u)
  $$
  This is a serious limitation as many relation types in multi-relational graphs are **directed and asymmetric.**

  改进**ComplEx** P43
  $$
  DEC(\pmb{z}_u,\tau,\pmb{z}_v) = Re(<\pmb{z}_u,\pmb{r}_\tau,\bar{\pmb{z}}_v>) = Re(\sum_{i=1}^d \pmb{z}_u[i] \times \pmb{r}_\tau [i] \times \bar{\pmb{z}}_v [i])
  $$
  where $\pmb{z}_u,\pmb{z}_v,\pmb{r}_\tau \in \mathbb{C}^d$ are complex-valued embeddings and Re denotes the real component of a complex vector.  Since we take the complex conjugate $\bar{\pmb{z}}_v$ of the tail embedding, this approach to decoding can accommodate asymmetric relations.

  另一种方法**RotatE**, defines the decoder as rotations in the complex plane as follows (P44)
  $$
  DEC(\pmb{z}_u,\tau,\pmb{z}_v) = -\|\pmb{z}_u \circ \pmb{r}_\tau - \pmb{z}_v \|
  $$
  where $\circ$ denotes the Hadamard product. 在上述公式中,we again assume that all embeddings are complex valued, and we additionally constrain the entries of $\pmb{r}_\tau$ so that $|\pmb{r}_\tau [i]| =1 ,\forall i \in \{1,...,d\}$. This restriction implies that each dimension of the relation embedding can be represented as $\pmb{r}_\tau[i] = e^{i \theta_{r,i}}$and thus corresponds to a rotation in the complex plane.

  

### 4.3.1  Representational abilities

描述各种多关系解码器的一种方法是根据它们在关系上表示不同逻辑模式的能力。

- **Symmetry and anti-symmetry** 

  many relations are *symmetric*, meaning that
  $$
  (u,\tau,v) \in \mathcal{E}  \leftrightarrow (v,\tau,u) \in \mathcal{E}
  $$
  we have explicitly *anti-symmetric* relations that satisfy:
  $$
  (u,\tau,v) \in \mathcal{E}  \rightarrow (v,\tau,u) \notin \mathcal{E}
  $$

- **Inversion**

  Related to symmetry is the notion of inversion, where one relation implies the existence of another, with opposite directionality:
  $$
  (u,\tau_1,v) \in \mathcal{E} \leftrightarrow (v,\tau_2,u) \in \mathcal{E}
  $$

- **Compositonality**

  we can consider whether or not the decoders can encode compositionality between relation representations of the form:
  $$
  (u,\tau_1,y) \in \mathcal{E} \land  (y,\tau_2,v) \in \mathcal{E} \rightarrow (u,\tau_3,v) \in \mathcal{E}
  $$
  For example, in TransE we can accommodate this by defining $\pmb{r}_{\tau_3} = \pmb{r}_{\tau_1}+ \pmb{r}_{\tau_2}$.  We can similarly model compositionality in RESCAL by defining $\pmb{R}_{\tau_3} = \pmb{R}_{\tau_2}\pmb{R}_{\tau_1}$ <font color=Red >**具体怎么对等的不是很清楚，感觉逻辑是对的**</font>

表4.2总结了我们讨论的各种解码器编码这些关系模式的能力。

<div align=center>
<img src="/Users/guass/Desktop/markdown/图神经网络/Graph Representation Learning/4png2.png" width="600" height=""/>
</div>
